{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from posixpath import join\n",
    "import numpy as np\n",
    "import random\n",
    "from numpy import count_nonzero as nnz\n",
    "from collections import defaultdict\n",
    "\n",
    "base_path = \"/mnt/datasets/uc5/UC5_pipeline_forked/experiments_eddl//mnt/datasets/uc5/UC5_pipeline_forked/experiments_eddl/eddl_ext_CNN_20tags\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAW\n",
    "inspect the \"report_raw\" file, containing the raw xml content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv( \"/mnt/datasets/uc5/UC5_pipeline_forked/experiments_eddl/reports_raw.tsv\", sep=\"\\t\", na_filter=False )\n",
    "print(f\"raw reports, shape {ds.shape}\")\n",
    "print(\"** columns:\")\n",
    "for c in ds.columns:\n",
    "    print(f\" - {c}\")\n",
    "\n",
    "# reports without images\n",
    "iii = ds.n_images == 0\n",
    "print(f\"*** number of reports without images {nnz(iii)}\")\n",
    "ds = ds.loc[~iii].reset_index()\n",
    "\n",
    "num_reports = ds.shape[0]\n",
    "num_images = ds.n_images.sum()\n",
    "print(f\"reports {num_reports}, images {num_images}\")\n",
    "\n",
    "img_g = ds.loc[:, [\"id\", \"n_images\"]].groupby([\"n_images\"]).agg([\"count\"])\n",
    "display(img_g)\n",
    "\n",
    "mm_g = ds[[\"id\", \"n_major_mesh\"]].groupby([\"n_major_mesh\"]).agg([\"count\"])\n",
    "display(mm_g)\n",
    "\n",
    "at_g = ds[[\"id\", \"n_auto_term\"]].groupby([\"n_auto_term\"]).agg([\"count\"])\n",
    "display(at_g)\n",
    "\n",
    "# remove reports without images\n",
    "del iii, img_g, mm_g, at_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>empty auto terms</h3>there are many reports with an empty set of auto terms. Check how many corresponds to normal reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_ids = ds.major_mesh == \"normal\"\n",
    "print(f\"normal reports (mesh): {nnz(normal_ids)}\")\n",
    "\n",
    "empty_auto_ids = ds.n_auto_term == 0\n",
    "normal_auto_ids = empty_auto_ids & normal_ids\n",
    "print(f\"empty auto terms that are normal according to mesh: {nnz(normal_auto_ids)}/{nnz(empty_auto_ids)}\")\n",
    "\n",
    "empty_auto_ids = empty_auto_ids & ~normal_auto_ids\n",
    "print(f\"empty auto terms that are not normal according to mesh: {nnz(empty_auto_ids)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combos\n",
    "unique combinations of terms (combos), on unprocessed, raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"major mesh, sample:\\n\\t\", random.sample(ds.major_mesh.tolist(), 5))\n",
    "print(\"auto term, sample:\\n\\t\", random.sample(ds.auto_term.tolist(), 5))\n",
    "\n",
    "def get_unique_set(col):\n",
    "    # out = [t.lower() for g in col for t in g.split(\";\") if len(g) > 0]\n",
    "    out = set()\n",
    "    for terms in col:\n",
    "        if len(terms) == 0:\n",
    "            continue\n",
    "        for t in terms.split(\";\"):\n",
    "            out.add(t.strip().lower())\n",
    "    return out\n",
    "\n",
    "u_mesh_combos = get_unique_set(ds.major_mesh)\n",
    "u_auto_combos = get_unique_set(ds.auto_term)\n",
    "\n",
    "print(f\"unique mesh combos: {len(u_mesh_combos)}\")\n",
    "print(f\"unique auto combos: {len(u_auto_combos)}\")\n",
    "\n",
    "print(\"unique mesh, sample:\", random.sample(u_mesh_combos, 10))\n",
    "print(\"unique auto, sample:\", random.sample(u_auto_combos, 10))\n",
    "\n",
    "mesh_in_empty_auto = get_unique_set(ds.major_mesh.loc[empty_auto_ids])\n",
    "print(f\"auto tags associated to non-normal empty auto terms: {len(mesh_in_empty_auto)} terms\")\n",
    "# print(mesh_in_empty_auto)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MeSH ONLY</h3>\n",
    "From now on, only mesh terms. \n",
    "\n",
    "Terms are made as it follows:\n",
    "HEADING/subheading;HEADING/subheading/...\n",
    "\n",
    "Simplify terms, keep only the HEADING terms and remote SUBHEADINS\n",
    "HEADING;HEADING;\n",
    "<b>some heading terms are repeated, in raw files repeated headings are specialized (thus being unique) with subeadings</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are no empty major mesh fields\n",
    "# expected syntax:\n",
    "import re\n",
    "print(\"sample of mesh terms\")\n",
    "print(random.sample(ds.major_mesh.tolist(), 3))\n",
    "\n",
    "# TODO\n",
    "# some headings terms appear more than once, ex:\n",
    "#      Calcified Granuloma/mediastinum/large;Calcified Granuloma/lung/hilum/right/large --> ['calcified granuloma', 'calcified granuloma']\n",
    "def simplify_terms(terms):\n",
    "    # split on ; [heading/subheadings, heading/subheading]\n",
    "    # then take only the first\n",
    "    out = [g.split(\"/\")[0].strip().lower() for g in terms.split(\";\")]  # heading/subheadings ; heading/subheadings ; \n",
    "    \n",
    "    # in some multi-word headings, words are separated by multiple spaces:\n",
    "    return list(set( [re.sub(r\"\\s+\", ' ', terms) for terms in out] ))\n",
    "\n",
    "simplified_mesh = ds.major_mesh.apply(lambda x: simplify_terms(x))\n",
    "# for m, s in zip(ds.major_mesh.tolist(), simplified_mesh.tolist()):\n",
    "#      print(f\" {m} --> {s}\")\n",
    "\n",
    "ds[\"labels\"] = simplified_mesh\n",
    "# labels_s: labels as string\n",
    "ds[\"labels_s\"] = simplified_mesh.apply(lambda x: \";\".join(x))  # labels joined as string\n",
    "ds[\"n_labels\"] = ds.labels.apply(lambda l: len(l))\n",
    "\n",
    "\n",
    "# the two \"counts\" that follow show different results because\n",
    "# some tags contain the same heading more than once followed by different subheadings (n_major_mesh)\n",
    "# in n_labels only unique headings are included\n",
    "mm_g = ds[[\"id\", \"n_major_mesh\"]].groupby([\"n_major_mesh\"]).agg([\"count\"])\n",
    "gnl = ds[[\"id\", \"n_labels\"]].groupby([\"n_labels\"]).agg([\"count\"])\n",
    "display(gnl)\n",
    "display(mm_g)\n",
    "\n",
    "\n",
    "print(\"number of unique combinations of terms:\", len(ds.labels_s.unique()))\n",
    "\n",
    "u_mesh = set()\n",
    "for l in ds.labels:\n",
    "    for v in l:\n",
    "        u_mesh.add(v)\n",
    "\n",
    "print(\"unique mesh terms: \", len(u_mesh))\n",
    "del mm_g, gnl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each term mark where it occurs\n",
    "\n",
    "occ_reps = {}\n",
    "occ_imgs = {}\n",
    "\n",
    "for ut in u_mesh:\n",
    "    reps = ds.labels.apply(lambda x: ut in x)\n",
    "    n_images = ds.n_images[reps].sum()\n",
    "    occ_reps[ut] = nnz(reps)\n",
    "    occ_imgs[ut] = n_images\n",
    "\n",
    "# for ut in u_mesh:\n",
    "#     print(f\"{ut}, {occ_reps[ut]} reports, {occ_imgs[ut]} images\")\n",
    "\n",
    "\n",
    "thresholds = [90, 100, 110, 120, 130]\n",
    "\n",
    "th2num = []\n",
    "for t in thresholds:\n",
    "    d = {}\n",
    "    for k, v in occ_imgs.items():\n",
    "        if v > t:\n",
    "            d[k] = v\n",
    "    th2num.append(d)\n",
    "\n",
    "for t, d in zip(thresholds, th2num):\n",
    "    print(f\"threshold {t}, number of tags {len(d)}\")\n",
    "    new_combos = ds.labels.apply(lambda l: [t for t in l if t in d.keys()])\n",
    "    len_new_combos = new_combos.apply(lambda x: len(x))\n",
    "    val_counts = len_new_combos.value_counts()\n",
    "    print(val_counts)\n",
    "    iii = len_new_combos > 1\n",
    "    jjj = len_new_combos == 0\n",
    "    print(f\"\\t {nnz(iii)}/{len(iii)} with multiple tags\")\n",
    "    print(f\"\\t {nnz(jjj)} without tags\")\n",
    "\n",
    "# last threshold\n",
    "print(ds.labels[iii])\n",
    "print(ds.labels[jjj])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a binary occurrence matrix\n",
    "\n",
    "# rep_tags = np.zeros( (ds.shape[0], len(u_mesh)), dtype=int)\n",
    "# img_tags = np.zeros( (n_images, len(u_mesh)), dtype=int)\n",
    "terms = sorted(u_mesh)\n",
    "terms = [\"normal\"] + [l for l in terms if l != \"normal\"]\n",
    "\n",
    "print(\"normal\" in terms)\n",
    "\n",
    "assert \"normal\" in terms \n",
    "assert (\"normal\" in terms[1:]) == False\n",
    "\n",
    "matrix = []  # rows correspond to reports\n",
    "rep_matrix = []  # rows correspond to reports\n",
    "index = []\n",
    "\n",
    "for t in ds.itertuples():\n",
    "    enc = []\n",
    "    for term in terms:\n",
    "        enc.append(term in t.labels)\n",
    "    rep_matrix.append(enc)\n",
    "    for i in [fn for fn in t.image_filename.split(\";\")]:\n",
    "        index.append(i)\n",
    "        matrix.append(enc)\n",
    "\n",
    "rep_ds = pd.DataFrame(data=rep_matrix, columns=terms)\n",
    "rep_ds[\"id\"] = ds.id\n",
    "print(f\"dataframe, index is report id: {rep_ds.shape}\")\n",
    "display(rep_ds)\n",
    "\n",
    "\n",
    "img_ds = pd.DataFrame(data=matrix, columns = terms)\n",
    "rep_ds[\"image_filename\"] = pd.Series(index)\n",
    "print(f\"dataframe, index is image_filename: {img_ds.shape}\")\n",
    "display(img_ds)\n",
    "# npterms = np.array(terms, dtype=object)\n",
    "# matrix = np.array(matrix)\n",
    "# idx = 3\n",
    "# print(matrix[idx,:])\n",
    "# print(npterms[matrix[idx,:]])\n",
    "            \n",
    "frequencies = img_ds.sum(axis=0)\n",
    "print(frequencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use img_ds\n",
    "def label_imbalance(df):\n",
    "    lab_n1 = df.sum(axis=0)\n",
    "    lab_n0 = df.shape[0] - lab_n1\n",
    "\n",
    "    lds = pd.concat([lab_n1, lab_n0], axis=1)\n",
    "    lds.columns = [\"n1\", \"n0\"]\n",
    "\n",
    "    # all of the following based on the fact that the minority class is 1\n",
    "    lds[\"IRLbl\"]  =  1/ lds.n1 * lds.n1.max(axis=0)\n",
    "    lds[\"ImR\"] = lds.n0 / lds.n1  # it is max / min, but here majority class is 0\n",
    "\n",
    "    lds[\"m_IRLbl\"] = lds.IRLbl.mean()\n",
    "    lds[\"m_ImR\"] = lds.ImR.mean()\n",
    "    return lds\n",
    "\n",
    "lab_imb = label_imbalance(img_ds)\n",
    "display(lab_imb)\n",
    "\n",
    "def drop_columns(df, columns):\n",
    "    df2 = df.drop(columns=columns)\n",
    "    n_labels = df2.sum(axis=1)\n",
    "    df2 = df2.drop(df.index[n_labels == 0])\n",
    "    return df2\n",
    "\n",
    "img_ds_others = drop_columns(img_ds, [\"normal\"])\n",
    "lab_imb_others = label_imbalance(img_ds_others)\n",
    "display(lab_imb_others)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# imbalance\n",
    "# tag to images\n",
    "lab2img = defaultdict(list)\n",
    "n_images = 0\n",
    "for row in ds.itertuples():\n",
    "    labels = row.labels\n",
    "    images = [fn for fn in row.image_filename.split(\";\")]\n",
    "    n_images += len(images)\n",
    "    for l in labels:\n",
    "        lab2img[l] += images\n",
    "\n",
    "print(f\"total number of images {n_images}, labels {len(lab2img)}\")\n",
    "records = []\n",
    "for k, v in lab2img.items():\n",
    "    #print(f\"{k}: {len(v)}\")\n",
    "    records.append({'label': k, 'n_1': len(v), 'n_0': (n_images-len(v))})\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "max_n_1 = df.n_1.max()\n",
    "df[\"IRLbl\"] = 1/df.n_1 * max_n_1\n",
    "df[\"ImR\"] = df.n_0 / df.n_1\n",
    "print(max_n_1)\n",
    "display(df)\n",
    "\n",
    "n_labels = df.shape[0]\n",
    "\n",
    "meanIR = df.IRLbl.sum() / n_labels\n",
    "print(meanIR)\n",
    "\n",
    "meanImR = df.ImR.sum() / n_labels\n",
    "\n",
    "import math\n",
    "CVIR = 1 / meanIR * math.sqrt(  ((df.IRLbl - meanIR)**2 / (n_labels - 1)).sum() )\n",
    "CVImR = 1 / meanImR * math.sqrt( ((df.ImR - meanImR)**2 / (n_labels - 1)).sum() )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.id[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: substitute synonims in the vocabulary\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "voc = pd.read_csv( join(\"text\", \"radiology_vocabulary_final.csv\"), na_filter=False)\n",
    "print(\"radiology vocabulary: \", voc.shape)\n",
    "print(voc.columns)\n",
    "\n",
    "term2syns = dict()\n",
    "syn2term = dict()\n",
    "\n",
    "syns_s = voc.apply(lambda row: [v.lower().strip() for v in row[3:] if (type(v) is str) and len(v)>0], axis=1)\n",
    "\n",
    "for term, syns in zip(voc[\"Term\"], syns_s):\n",
    "    term2syns[term.lower().strip()] = syns\n",
    "    for syn in syns:\n",
    "        syn2term[syn] = term.lower()\n",
    "\n",
    "random_keys = random.sample(list(term2syns.keys()), 5)\n",
    "for key in random_keys:\n",
    "    print(f\" - {key}: {term2syns[key]}\")\n",
    "\n",
    "# Check which terms are not present in the vocabulary (read from disk in the cell above)\n",
    "missing_terms = []\n",
    "for i, t in enumerate(sorted(u_mesh)):\n",
    "    ok = t in term2syns.keys()\n",
    "    if not ok:\n",
    "        missing_terms.append(t)\n",
    "    # print(f\"{i:03}. {t} - ok? {ok}\")\n",
    "\n",
    "\n",
    "# they should be two \"no indexing\" and \"normal\"\n",
    "print(f\"missing terms: {len(missing_terms)}\")\n",
    "for t in missing_terms:\n",
    "    print(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8ac5ee7dbd3222b9a621db727279133c0b9b65990c5851472ddfcb34807bcee"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('eddl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
