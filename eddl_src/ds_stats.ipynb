{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from posixpath import join\n",
    "import numpy as np\n",
    "import random\n",
    "from numpy import count_nonzero as nnz\n",
    "from collections import defaultdict\n",
    "from IPython.display import display\n",
    "\n",
    "base_path = \"/mnt/datasets/uc5/UC5_pipeline_forked/experiments_eddl//mnt/datasets/uc5/UC5_pipeline_forked/experiments_eddl/eddl_ext_CNN_20tags\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAW\n",
    "inspect the \"report_raw\" file, containing the raw xml content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv( \"/mnt/datasets/uc5/UC5_pipeline_forked/experiments_eddl/reports_raw.tsv\", sep=\"\\t\", na_filter=False )\n",
    "print(f\"raw reports, shape {ds.shape}\")\n",
    "print(\"** columns:\")\n",
    "for c in ds.columns:\n",
    "    print(f\" - {c}\")\n",
    "\n",
    "# reports without images\n",
    "iii = ds.n_images == 0\n",
    "print(f\"*** number of reports without images {nnz(iii)}\")\n",
    "ds = ds.loc[~iii].reset_index()\n",
    "\n",
    "num_reports = ds.shape[0]\n",
    "num_images = ds.n_images.sum()\n",
    "print(f\"reports {num_reports}, images {num_images}\")\n",
    "\n",
    "img_g = ds.loc[:, [\"id\", \"n_images\"]].groupby([\"n_images\"]).agg([\"count\"])\n",
    "display(img_g)\n",
    "\n",
    "mm_g = ds[[\"id\", \"n_major_mesh\"]].groupby([\"n_major_mesh\"]).agg([\"count\"])\n",
    "display(mm_g)\n",
    "\n",
    "at_g = ds[[\"id\", \"n_auto_term\"]].groupby([\"n_auto_term\"]).agg([\"count\"])\n",
    "display(at_g)\n",
    "\n",
    "# remove reports without images\n",
    "del iii, img_g, mm_g, at_g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>empty auto terms</h3>there are many reports with an empty set of auto terms. Check how many corresponds to normal reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_ids = ds.major_mesh == \"normal\"\n",
    "print(f\"normal reports (mesh): {nnz(normal_ids)}\")\n",
    "\n",
    "empty_auto_ids = ds.n_auto_term == 0\n",
    "normal_auto_ids = empty_auto_ids & normal_ids\n",
    "print(f\"empty auto terms that are normal according to mesh: {nnz(normal_auto_ids)}/{nnz(empty_auto_ids)}\")\n",
    "\n",
    "empty_auto_ids = empty_auto_ids & ~normal_auto_ids\n",
    "print(f\"empty auto terms that are not normal according to mesh: {nnz(empty_auto_ids)}\")\n",
    "\n",
    "empty_auto_ids = ds.n_auto_term == 0\n",
    "normal_auto_ids = empty_auto_ids & normal_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combos\n",
    "unique combinations of terms (combos), on unprocessed, raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"major mesh, sample:\\n\\t\", random.sample(ds.major_mesh.tolist(), 5))\n",
    "print(\"auto term, sample:\\n\\t\", random.sample(ds.auto_term.tolist(), 5))\n",
    "\n",
    "def get_unique_set(col):\n",
    "    # out = [t.lower() for g in col for t in g.split(\";\") if len(g) > 0]\n",
    "    out = set()\n",
    "    for terms in col:\n",
    "        if len(terms) == 0:\n",
    "            continue\n",
    "        for t in terms.split(\";\"):\n",
    "            out.add(t.strip().lower())\n",
    "    return out\n",
    "\n",
    "u_mesh_combos = get_unique_set(ds.major_mesh)\n",
    "u_auto_combos = get_unique_set(ds.auto_term)\n",
    "\n",
    "print(f\"unique mesh combos: {len(u_mesh_combos)}\")\n",
    "print(f\"unique auto combos: {len(u_auto_combos)}\")\n",
    "\n",
    "print(\"unique mesh, sample:\", random.sample(u_mesh_combos, 10))\n",
    "print(\"unique auto, sample:\", random.sample(u_auto_combos, 10))\n",
    "\n",
    "mesh_in_empty_auto = get_unique_set(ds.major_mesh.loc[empty_auto_ids])\n",
    "print(f\"auto tags associated to non-normal empty auto terms: {len(mesh_in_empty_auto)} terms\")\n",
    "# print(mesh_in_empty_auto)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MeSH ONLY</h3>\n",
    "From now on, only mesh terms. \n",
    "\n",
    "Terms are made as it follows:\n",
    "HEADING/subheading;HEADING/subheading/...\n",
    "\n",
    "Simplify terms, keep only the HEADING terms and remote SUBHEADINS\n",
    "HEADING;HEADING;\n",
    "<b>some heading terms are repeated, in raw files repeated headings are specialized (thus being unique) with subeadings</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are no empty major mesh fields\n",
    "# expected syntax:\n",
    "import re\n",
    "print(\"sample of mesh terms\")\n",
    "print(random.sample(ds.major_mesh.tolist(), 3))\n",
    "\n",
    "# TODO\n",
    "# some headings terms appear more than once, ex:\n",
    "#      Calcified Granuloma/mediastinum/large;Calcified Granuloma/lung/hilum/right/large --> ['calcified granuloma', 'calcified granuloma']\n",
    "def simplify_terms(terms):\n",
    "    # split on ; [heading/subheadings, heading/subheading]\n",
    "    # then take only the first\n",
    "    out = [g.split(\"/\")[0].strip().lower() for g in terms.split(\";\")]  # heading/subheadings ; heading/subheadings ; \n",
    "    \n",
    "    # in some multi-word headings, words are separated by multiple spaces:\n",
    "    return list(set( [re.sub(r\"\\s+\", ' ', terms) for terms in out] ))\n",
    "\n",
    "simplified_mesh = ds.major_mesh.apply(lambda x: simplify_terms(x))\n",
    "# for m, s in zip(ds.major_mesh.tolist(), simplified_mesh.tolist()):\n",
    "#      print(f\" {m} --> {s}\")\n",
    "\n",
    "ds[\"labels\"] = simplified_mesh\n",
    "# labels_s: labels as string\n",
    "ds[\"labels_s\"] = simplified_mesh.apply(lambda x: \";\".join(x))  # labels joined as string\n",
    "ds[\"n_labels\"] = ds.labels.apply(lambda l: len(l))\n",
    "\n",
    "\n",
    "# the two \"counts\" that follow show different results because\n",
    "# some tags contain the same heading more than once followed by different subheadings (n_major_mesh)\n",
    "# in n_labels only unique headings are included\n",
    "mm_g = ds[[\"id\", \"n_major_mesh\"]].groupby([\"n_major_mesh\"]).agg([\"count\"])\n",
    "gnl = ds[[\"id\", \"n_labels\"]].groupby([\"n_labels\"]).agg([\"count\"])\n",
    "display(gnl)\n",
    "display(mm_g)\n",
    "\n",
    "u_mesh = set()\n",
    "for l in ds.labels:\n",
    "    for v in l:\n",
    "        u_mesh.add(v)\n",
    "\n",
    "print(\"unique mesh terms: \", len(u_mesh))\n",
    "del mm_g, gnl\n",
    "\n",
    "\n",
    "unique_mesh_combos = ds.labels_s.value_counts()\n",
    "print(\"unique mesh combinations:\", len(unique_mesh_combos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each term mark where it occurs\n",
    "\n",
    "occ_reps = {}\n",
    "occ_imgs = {}\n",
    "\n",
    "for ut in u_mesh:\n",
    "    reps = ds.labels.apply(lambda x: ut in x)\n",
    "    n_images = ds.n_images[reps].sum()\n",
    "    occ_reps[ut] = nnz(reps)\n",
    "    occ_imgs[ut] = n_images\n",
    "\n",
    "# for ut in u_mesh:\n",
    "#     print(f\"{ut}, {occ_reps[ut]} reports, {occ_imgs[ut]} images\")\n",
    "\n",
    "\n",
    "thresholds = [90, 100, 110, 120, 130]\n",
    "\n",
    "th2num = []\n",
    "for t in thresholds:\n",
    "    d = {}\n",
    "    for k, v in occ_imgs.items():\n",
    "        if v > t:\n",
    "            d[k] = v\n",
    "    th2num.append(d)\n",
    "\n",
    "for t, d in zip(thresholds, th2num):\n",
    "    print(f\"threshold {t}, number of tags {len(d)}\")\n",
    "    new_combos = ds.labels.apply(lambda l: [t for t in l if t in d.keys()])\n",
    "    len_new_combos = new_combos.apply(lambda x: len(x))\n",
    "    val_counts = len_new_combos.value_counts()\n",
    "    print(val_counts)\n",
    "    iii = len_new_combos > 1\n",
    "    jjj = len_new_combos == 0\n",
    "    print(f\"\\t {nnz(iii)}/{len(iii)} with multiple tags\")\n",
    "    print(f\"\\t {nnz(jjj)} without tags\")\n",
    "\n",
    "# last threshold\n",
    "print(ds.labels[iii])\n",
    "print(ds.labels[jjj])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a binary occurrence matrix\n",
    "\n",
    "# rep_tags = np.zeros( (ds.shape[0], len(u_mesh)), dtype=int)\n",
    "# img_tags = np.zeros( (n_images, len(u_mesh)), dtype=int)\n",
    "terms = sorted(u_mesh)\n",
    "terms = [\"normal\"] + [l for l in terms if l != \"normal\"]\n",
    "\n",
    "print(\"normal\" in terms)\n",
    "\n",
    "assert \"normal\" in terms \n",
    "assert (\"normal\" in terms[1:]) == False\n",
    "\n",
    "matrix = []  # rows correspond to reports\n",
    "rep_matrix = []  # rows correspond to reports\n",
    "index = []\n",
    "\n",
    "for t in ds.itertuples():\n",
    "    enc = []\n",
    "    for term in terms:\n",
    "        enc.append(term in t.labels)\n",
    "    rep_matrix.append(enc)\n",
    "    for i in [fn for fn in t.image_filename.split(\";\")]:\n",
    "        index.append(i)\n",
    "        matrix.append(enc)\n",
    "\n",
    "rep_ds = pd.DataFrame(data=np.array(rep_matrix).astype(int), columns=terms)\n",
    "rep_ds[\"id\"] = ds.id\n",
    "print(f\"dataframe, index is report id: {rep_ds.shape}\")\n",
    "display(rep_ds)\n",
    "\n",
    "\n",
    "img_ds = pd.DataFrame(data=np.array(matrix).astype(int), columns = terms)\n",
    "img_ds[\"image_filename\"] = pd.Series(index)\n",
    "print(f\"dataframe, index is image_filename: {img_ds.shape}\")\n",
    "img_ds = img_ds.set_index([\"image_filename\"])\n",
    "display(img_ds)\n",
    "# npterms = np.array(terms, dtype=object)\n",
    "# matrix = np.array(matrix)\n",
    "# idx = 3\n",
    "# print(matrix[idx,:])\n",
    "# print(npterms[matrix[idx,:]])\n",
    "            \n",
    "label_counts = img_ds.sum(axis=0)\n",
    "\n",
    "n_labels = img_ds.sum(axis=1)\n",
    "print(n_labels.value_counts())\n",
    "\n",
    "iii = n_labels == 12\n",
    "rows = img_ds.loc[iii]\n",
    "a = np.array(rows).astype(bool)\n",
    "  \n",
    "npterms = np.array(terms)\n",
    "for i in range(a.shape[0]):\n",
    "    # print(a[i])\n",
    "    print( npterms[a[i]].tolist() )\n",
    "   \n",
    "\n",
    "img_ds.to_csv(\"/mnt/datasets/uc5/std-dataset/img_ds_no_text.tsv\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 0, 0], [1, 0, 0], [0,1,1], [0,1,0], [0,0,1]]\n",
    "d = pd.DataFrame(data=data, columns=[\"f1\", \"f2\", \"f3\"], index=['a', 'b', 'c', 'd', 'e'])\n",
    "display(d)\n",
    "d.loc[d[\"f1\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a dataset of normal vs non-normal images\n",
    "# display(img_ds)\n",
    "sorted_labels = label_counts.sort_values(ascending=False)\n",
    "\n",
    "normal = img_ds.loc[img_ds[\"normal\"]==1] \n",
    "n_normal = normal.shape[0]\n",
    "no_normal = img_ds.loc[img_ds[\"normal\"]==0].sample(len(normal), random_state=11, axis=0)\n",
    "\n",
    "print(f\"normal, shape: {normal.shape}\")\n",
    "print(f\"rest, shape: {no_normal.shape}\")\n",
    "\n",
    "#display(normal)\n",
    "#display(no_normal)\n",
    "\n",
    "nolabel_counts = no_normal.sum(axis=0)\n",
    "stats = pd.DataFrame()\n",
    "stats[\"original\"] = label_counts\n",
    "stats[\"selected\"] = nolabel_counts\n",
    "display(stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([True, False, True])\n",
    "print(a[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to normal, aorta, and abdomen\n",
    "sel_cols = ['normal', 'aorta, thoracic', 'bone diseases, metabolic', 'calcinosis', 'cardiomegaly', 'cicatrix', 'costophrenic angle', 'deformity', 'fractures, bone', 'opacity', 'pleural effusion', 'pulmonary atelectasis', 'spine']\n",
    "rows_idxs = np.zeros( (img_ds.shape[0],), dtype=bool)\n",
    "for c in sel_cols:\n",
    "    idxs = img_ds.loc[:, c] == 1\n",
    "    rows_idxs = rows_idxs | idxs\n",
    "    \n",
    "\n",
    "# row_idxs =  (img_ds.abdomen == 1)  # (img_ds.normal == 1) | (img_ds.aorta == 1) |\n",
    "img_ds2 = img_ds.drop(img_ds.loc[~rows_idxs].index)\n",
    "img_ds2 = img_ds2.drop(columns=[c for c in img_ds2.columns if c not in sel_cols])\n",
    "display(img_ds2)\n",
    "\n",
    "img_ds2 = img_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc = [ [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1]]\n",
    "test_ds = pd.DataFrame(data=np.array(enc), columns=[\"l1\", \"l2\"])\n",
    "\n",
    "# use img_ds\n",
    "def label_imbalance(df):\n",
    "    lab_n1 = df.sum(axis=0)\n",
    "    lab_n0 = df.shape[0] - lab_n1\n",
    "\n",
    "    lds = pd.concat([lab_n1, lab_n0], axis=1)\n",
    "    lds.columns = [\"n1\", \"n0\"]\n",
    "    lds.index = lds.index.rename(\"label\")\n",
    "\n",
    "    # all of the following based on the fact that the minority class is 1\n",
    "    lds[\"IRLbl\"]  = lds.n1.max(axis=0) / lds.n1\n",
    "    lds[\"ImR\"] = lds.n0 / lds.n1  # it is max / min, but here majority class is 0\n",
    "\n",
    "    mean_irlbl = lds.IRLbl.mean()\n",
    "    lds[\"m_IRLbl\"] = mean_irlbl\n",
    "    lds[\"m_ImR\"] = lds.ImR.mean()\n",
    "    \n",
    "    # variation\n",
    "    sigma =  (lds.IRLbl - mean_irlbl)**2  \n",
    "    sigma = sigma.sum()\n",
    "    sigma = sigma / (lds.shape[0]-1)\n",
    "    sigma = np.sqrt(sigma)\n",
    "    print(\"sigma:\", sigma.shape)\n",
    "    cvir = sigma / mean_irlbl\n",
    "    print(\"cvir\", cvir)\n",
    "    lds[\"CVIR\"] = cvir\n",
    "\n",
    "\n",
    "    return lds\n",
    "\n",
    "lab_imb = label_imbalance(test_ds)\n",
    "with pd.option_context('display.max_rows', 120, 'display.max_columns', 10):\n",
    "    display(lab_imb.reset_index())\n",
    "\n",
    "def drop_columns(df, columns):\n",
    "    df2 = df.drop(columns=columns)\n",
    "    n_labels = df2.sum(axis=1)\n",
    "    df2 = df2.drop(df.index[n_labels == 0])\n",
    "    display(df2)\n",
    "    return df2\n",
    "\n",
    " \n",
    "# img_ds_others = drop_columns(img_ds2, [\"normal\"])\n",
    "# lab_imb_others = label_imbalance(img_ds_others)\n",
    "# display(lab_imb_others)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_scumble(ds, li, dev=True):\n",
    "    enc = ds.to_numpy()  # rows: images, columns: labels (1-hot encoded)\n",
    "    n_labels = np.sum(enc, axis=1)  # vectrized, number of labels per image lambda i: SUM_{j over labels} y_{ij}\n",
    "    freqs = np.sum(enc, axis=0)\n",
    "    irlbl = li.IRLbl.to_numpy()  # per label IRLbl measure\n",
    "\n",
    "    if dev:\n",
    "        print(\"enc:\", enc.shape)\n",
    "        print(\"irlbl:\", irlbl.shape)\n",
    "        print(\"n_labels:\", n_labels.shape)\n",
    "        # print(irlbl)\n",
    "\n",
    "    # first step, \n",
    "    # for each image, prod of the irlbl of the labels\n",
    "    irlbl_1 = irlbl.reshape( (1, -1) )\n",
    "    product = np.multiply(enc, np.repeat(irlbl_1, enc.shape[0], axis=0))\n",
    "    print(\"product:\", product)\n",
    "    product2 = np.where(product != 0, product, 1)\n",
    "    print(\"product2, reduces:\", product2)\n",
    "\n",
    "    P = product2.prod(axis=1)\n",
    "    P = P**(1.0 / enc.shape[1])\n",
    "    m_irlbl_img = product.mean(axis=1) \n",
    "    print(\"mean irlbl for image:\",m_irlbl_img)\n",
    "\n",
    "    P = P / m_irlbl_img\n",
    "    print(\"P norm:\", P)\n",
    "    scumble_ins = 1 - P\n",
    "    scumble = scumble_ins.mean()\n",
    "\n",
    "    # variation\n",
    "    squared = (scumble_ins - scumble)**2 / (ds.shape[0] - 1)\n",
    "    scumble_sigma = np.sqrt(squared.sum())\n",
    "    scumble_cv = scumble_sigma/scumble if scumble > 0 else 0\n",
    "\n",
    "    # scumble_lbl\n",
    "    scumble_ins_1 = scumble_ins.reshape( (-1, 1))\n",
    "    num = np.multiply(enc, scumble_ins_1)\n",
    "    num = num.sum(axis=0)\n",
    "    scumble_lbl = np.divide(num, freqs)\n",
    "\n",
    "    return scumble, scumble_ins, scumble_cv, scumble_lbl\n",
    "\n",
    "\n",
    "scumble, scumble_ins, scumble_cv, scumble_lbl = compute_scumble(test_ds, lab_imb)\n",
    "\n",
    "print(scumble_ins)\n",
    "\n",
    "print(\"scumble:\", scumble)\n",
    "print(\"scumble_ins:\", scumble_ins.shape)\n",
    "print(\"scumble_cv:\", scumble_cv)\n",
    "print(\"scumble_lbl:\", scumble_lbl)\n",
    "\n",
    "lab_imb[\"scumble\"] = scumble\n",
    "lab_imb[\"scumble_cv\"] = scumble_cv\n",
    "lab_imb[\"scumble_lbl\"] = scumble_lbl\n",
    "\n",
    "display(lab_imb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX OLD\n",
    "# imbalance\n",
    "# tag to images\n",
    "lab2img = defaultdict(list)\n",
    "n_images = 0\n",
    "for row in ds.itertuples():\n",
    "    labels = row.labels\n",
    "    images = [fn for fn in row.image_filename.split(\";\")]\n",
    "    n_images += len(images)\n",
    "    for l in labels:\n",
    "        lab2img[l] += images\n",
    "\n",
    "print(f\"total number of images {n_images}, labels {len(lab2img)}\")\n",
    "records = []\n",
    "for k, v in lab2img.items():\n",
    "    #print(f\"{k}: {len(v)}\")\n",
    "    records.append({'label': k, 'n_1': len(v), 'n_0': (n_images-len(v))})\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "max_n_1 = df.n_1.max()\n",
    "df[\"IRLbl\"] = 1/df.n_1 * max_n_1\n",
    "df[\"ImR\"] = df.n_0 / df.n_1\n",
    "print(max_n_1)\n",
    "display(df)\n",
    "\n",
    "n_labels = df.shape[0]\n",
    "\n",
    "meanIR = df.IRLbl.sum() / n_labels\n",
    "print(meanIR)\n",
    "\n",
    "meanImR = df.ImR.sum() / n_labels\n",
    "\n",
    "import math\n",
    "CVIR = 1 / meanIR * math.sqrt(  ((df.IRLbl - meanIR)**2 / (n_labels - 1)).sum() )\n",
    "CVImR = 1 / meanImR * math.sqrt( ((df.ImR - meanImR)**2 / (n_labels - 1)).sum() )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([0.1, 0.2, 0.3]).reshape((-1,1))\n",
    "c = np.repeat(b, 3, axis=1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.id[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: substitute synonims in the vocabulary\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "voc = pd.read_csv( join(\"text\", \"radiology_vocabulary_final.csv\"), na_filter=False)\n",
    "print(\"radiology vocabulary: \", voc.shape)\n",
    "print(voc.columns)\n",
    "\n",
    "term2syns = dict()\n",
    "syn2term = dict()\n",
    "\n",
    "syns_s = voc.apply(lambda row: [v.lower().strip() for v in row[3:] if (type(v) is str) and len(v)>0], axis=1)\n",
    "\n",
    "for term, syns in zip(voc[\"Term\"], syns_s):\n",
    "    term2syns[term.lower().strip()] = syns\n",
    "    for syn in syns:\n",
    "        syn2term[syn] = term.lower()\n",
    "\n",
    "random_keys = random.sample(list(term2syns.keys()), 5)\n",
    "for key in random_keys:\n",
    "    print(f\" - {key}: {term2syns[key]}\")\n",
    "\n",
    "# Check which terms are not present in the vocabulary (read from disk in the cell above)\n",
    "missing_terms = []\n",
    "for i, t in enumerate(sorted(u_mesh)):\n",
    "    ok = t in term2syns.keys()\n",
    "    if not ok:\n",
    "        missing_terms.append(t)\n",
    "    # print(f\"{i:03}. {t} - ok? {ok}\")\n",
    "\n",
    "\n",
    "# they should be two \"no indexing\" and \"normal\"\n",
    "print(f\"missing terms: {len(missing_terms)}\")\n",
    "for t in missing_terms:\n",
    "    print(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8ac5ee7dbd3222b9a621db727279133c0b9b65990c5851472ddfcb34807bcee"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('eddl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
