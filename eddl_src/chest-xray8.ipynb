{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# CHEST-XRAY8\n",
    "# notebook for stats and building training sets\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "from numpy import count_nonzero as nnz\n",
    "import pandas as pd\n",
    "from posixpath import join\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "base_fld = \"/mnt/datasets/mimic-cxr/chestx-ray8\"\n",
    "shuffle_seed = 2  # for repeatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#d07326\">PART I </font>\n",
    "<font color=\"#21b3d2\">PREPROCESS RAW DATASET</text>\n",
    "\n",
    "output: multi-label (1-hot) encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# dataset\n",
    "# \n",
    "\n",
    "#> metadata and labels for each image:\n",
    "datae = pd.read_csv( join(base_fld, \"Data_Entry_2017_v2020.csv\"), index_col=\"Image Index\")\n",
    "print(\"Data_Entry_2017_v2020.csv, shape:\", datae.shape)\n",
    "print(datae.columns)\n",
    "display(datae)\n",
    "#<\n",
    "\n",
    "# > View position\n",
    "print(\"View position:\")\n",
    "print(datae.loc[:, \"View Position\"].value_counts())  # AP or PA\n",
    "# <\n",
    "\n",
    "# > labels (classes)\n",
    "lab_col = datae.loc[:, \"Finding Labels\"].value_counts()  # classes\n",
    "# combinations of labels with form: Effusion|Infiltration, separated by |\n",
    "print(\"unique label combos:\", len(lab_col))\n",
    "\n",
    "# single labels\n",
    "labels = set()\n",
    "datae.loc[:, \"Finding Labels\"].apply(lambda x: labels.update([y for y in x.split(\"|\")]))\n",
    "labels = sorted(labels)\n",
    "print(f\"unique labels ({len(labels)}):\", labels)\n",
    "# <\n",
    "\n",
    "# > used for binary encoding the labels\n",
    "l2i, i2l = {}, {}\n",
    "for i, l in enumerate(labels):\n",
    "    l2i[l] = i\n",
    "    i2l[str(i)] = l\n",
    "# <"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# dataset\n",
    "# \n",
    "\n",
    "#> metadata and labels for each image:\n",
    "datae = pd.read_csv( join(base_fld, \"Data_Entry_2017_v2020.csv\"), index_col=\"Image Index\")\n",
    "print(\"Data_Entry_2017_v2020.csv, shape:\", datae.shape)\n",
    "print(datae.columns)\n",
    "display(datae)\n",
    "#<\n",
    "\n",
    "# > View position\n",
    "print(\"View position:\")\n",
    "print(datae.loc[:, \"View Position\"].value_counts())  # AP or PA\n",
    "# <\n",
    "\n",
    "# > labels (classes)\n",
    "lab_col = datae.loc[:, \"Finding Labels\"].value_counts()  # classes\n",
    "# combinations of labels with form: Effusion|Infiltration, separated by |\n",
    "print(\"unique label combos:\", len(lab_col))\n",
    "\n",
    "# single labels\n",
    "labels = set()\n",
    "datae.loc[:, \"Finding Labels\"].apply(lambda x: labels.update([y for y in x.split(\"|\")]))\n",
    "labels = sorted(labels)\n",
    "print(f\"unique labels ({len(labels)}):\", labels)\n",
    "# <\n",
    "\n",
    "# > used for binary encoding the labels\n",
    "l2i, i2l = {}, {}\n",
    "for i, l in enumerate(labels):\n",
    "    l2i[l] = i\n",
    "    i2l[str(i)] = l\n",
    "# <"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode labelling as a binary matrix n_images x n_labels:\n",
    "#  - rows -> images\n",
    "#  - columns -> labels\n",
    "filenames = []\n",
    "encoding = np.zeros( (datae.shape[0], len(labels)), dtype=int) # bool mat encoding labels\n",
    "for r, t in enumerate(datae.loc[:, [\"Finding Labels\"]].itertuples()):\n",
    "    filenames.append(t[0])\n",
    "    labs = t[1].split(\"|\")\n",
    "    for l in labs:\n",
    "        encoding[r, l2i[l]] = 1\n",
    "\n",
    "df = pd.DataFrame(data = encoding, columns=labels, index=pd.Series(filenames))\n",
    "# print(datae.loc['00030801_001.png', \"Finding Labels\"])  # check\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# in the original labels only 8 labels 'paper_labels' were used\n",
    "#  the \"No finding\" and the other labels were \"collapsed\" in a 0-vector\n",
    "\n",
    "# >\n",
    "paper_labels = [\"Atelectasis\", \"Cardiomegaly\", \"Effusion\", \"Infiltration\", \"Mass\", \"Nodule\", \"Pneumonia\", \"Pneumothorax\"]\n",
    "paper_labels = sorted([\"No Finding\"] + paper_labels)  # add a label for \"No Finding\"\n",
    "\n",
    "# check\n",
    "# there is an error in the paper Pneumathorax, corrected in the labels above. the following check should pass\n",
    "for pl in paper_labels:\n",
    "    assert pl in labels\n",
    "# <\n",
    "\n",
    "# > columns not used in the original paper: substitute all with 'other', populated with logical_or(other_columns)\n",
    "other_columns = [c for c in df.columns if (c not in paper_labels)]\n",
    "other_df = df.loc[:, other_columns]\n",
    "other_col = other_df.sum(axis=1)\n",
    "other_col.where(other_col > 0, 1)  # set to 1(true) where one of those label occurs\n",
    "df[\"other\"] = other_col\n",
    "paper_labels = paper_labels + [\"other\"]\n",
    "# <\n",
    "\n",
    "# >\n",
    "old_shape = df.shape\n",
    "df = df.drop(columns=other_columns)  #  + [\"other\"])  # remove columns\n",
    "print(f\"new shape of dataset {old_shape} ->\", df.shape)\n",
    "del old_shape\n",
    "print(df.columns)\n",
    "# < \n",
    "\n",
    "# > counts\n",
    "frequencies = df.sum(axis=0)\n",
    "n_labels = df.sum(axis=1)\n",
    "assert nnz(n_labels==0) == 0  # no row without at least one label\n",
    "# <\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# dataset comes with a standard split train - test\n",
    "#\n",
    "\n",
    "# check train_val_list and test_list\n",
    "# 1. image exists in folder?\n",
    "# 2. image exists in dataset?\n",
    "def read_list(fn):\n",
    "    with open( fn, \"r\") as fin:\n",
    "        ids = fin.readlines()\n",
    "    return ids\n",
    "\n",
    "# >\n",
    "# this list would be filled with index values corresponding to image without labels\n",
    "#       when the \"other\" column is not added. Now all the images have labels.\n",
    "removed_filenames = []  \n",
    "\n",
    "train_val = [s.strip() for s in read_list( join(base_fld, \"train_val_list.txt\") )]\n",
    "train_val = [s for s in train_val if (s not in removed_filenames)]\n",
    "\n",
    "test = [s.strip() for s in read_list(join (join(base_fld, \"test_list.txt\")))]\n",
    "test = [s for s in test if (s not in removed_filenames)]\n",
    "\n",
    "print(f\"train&validation: {len(train_val)}\")\n",
    "print(f\"test: {len(test)}\")\n",
    "print(f\"total: {len(train_val)+len(test)}\")\n",
    "assert len(train_val) + len(test) == df.shape[0] # + nnz(with_zero_labels)  # add the rows removed\n",
    "# <\n",
    "\n",
    "# > check\n",
    "# all images have been downloaded? PASSED\n",
    "#import os\n",
    "#for r in ds.iterrows():\n",
    "#    fn = r[0]\n",
    "#    assert os.path.exists( join(base_fld, \"images\", fn) )\n",
    "# DONE: all images exists\n",
    "# <\n",
    "\n",
    "# > split column (df index is filename)\n",
    "df[\"split\"] = None\n",
    "df.loc[train_val, \"split\"] = \"train\"\n",
    "df.loc[test, \"split\"] = \"test\"\n",
    "df.index.name = \"Image Index\"\n",
    "# <\n",
    "\n",
    "df.to_csv( join(base_fld, \"chest-xray8_uc5.tsv\"), sep=\"\\t\")\n",
    "datae.to_csv( join(base_fld, \"chest-xray8_uc5_encoding.tsv\"), sep=\"\\t\")\n",
    "print(\"dataset saved, location:\", join(base_fld, \"chest-xray8_uc5.tsv\"))\n",
    "print(\"encoding saved at:\", join(base_fld, \"chest-xray8_uc5_encoding.tsv\"))\n",
    "\n",
    " # <<< FIRST PART ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#d07326\">PART II </font>\n",
    "<font color=\"#21b3d2\">INDIPENDENT FROM HERE (but for imports and base_fld): USING RESUTS OF THE CELLS ABOVE</text>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">PREPARE CROSS_VALIDATION</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train, valid and test for Positive and Negative\n",
    "# keep it balanced\n",
    "\n",
    "#>\n",
    "ds = pd.read_csv( join(base_fld, \"chest-xray8_uc5.tsv\"), sep=\"\\t\", index_col=\"Image Index\" )\n",
    "datae = pd.read_csv( join(base_fld, \"Data_Entry_2017_v2020.csv\"), index_col=\"Image Index\")\n",
    "\n",
    "ds[\"View Position\"] = datae[\"View Position\"]  # same index\n",
    "display(ds)\n",
    "\n",
    "print(\"files read:\")\n",
    "print(f\"dataset: {ds.shape}\")\n",
    "print(f\"\\t - with columns: {ds.columns}\")\n",
    "\n",
    "# >\n",
    "train = ds[ds.split == \"train\"]\n",
    "training_set = train.groupby([\"No Finding\", \"View Position\"]).apply(lambda x: x.sample( int((4000 + 1000) / 4), random_state=shuffle_seed) )\n",
    "test = ds[ds.split == \"test\"]\n",
    "test_set = test.groupby([\"No Finding\", \"View Position\"]).apply(lambda x: x.sample( int(2000/4), random_state=shuffle_seed) )\n",
    "# <\n",
    "\n",
    "# >\n",
    "# ViewPosition and NoFinding are both in the index and in the columns\n",
    "def filter_columns(df):\n",
    "    df = df.drop(columns=[\"View Position\", \"No Finding\"])  # after groupby the two columns are also in the index...\n",
    "    df = df.reset_index().set_index(\"Image Index\")  # ...so reset_index() would fail if not dropped \n",
    "    df = df.drop(columns=[c for c in df.columns if (c not in paper_labels)])\n",
    "    return df\n",
    "\n",
    "training_set = filter_columns(training_set)\n",
    "test_set = filter_columns(test_set)\n",
    "# <\n",
    "\n",
    "# > check no intersection\n",
    "def common_index(d1, d2):\n",
    "    idx = d1.index.isin(d2.index)\n",
    "    return idx\n",
    "\n",
    "assert nnz(common_index(training_set, test_set)) ==0 \n",
    "# <\n",
    "\n",
    "# > check: PASSED\n",
    "# print(test_set.iloc[0])\n",
    "# key = \"00028856_000.png\"\n",
    "# print (key in train.index.values)\n",
    "# print (key in test.index.values)\n",
    "\n",
    "# print(ds.loc[key])\n",
    "# <\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "target = \"No Finding\"\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=shuffle_seed)\n",
    "X = training_set.index.to_numpy()\n",
    "y = training_set[target].to_numpy()\n",
    "\n",
    "for i, (train_idx, valid_idx) in skf.split(training_set, y, stratify=training_set[\"No Finding\", \"View Position\"]):\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">N_ITER BOOTSTRAPS</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train, valid and test for Positive and Negative\n",
    "# keep it balanced\n",
    "\n",
    "# >\n",
    "ds = pd.read_csv( join(base_fld, \"chest-xray8_uc5.tsv\"), sep=\"\\t\", index_col=\"Image Index\" )\n",
    "datae = pd.read_csv( join(base_fld, \"Data_Entry_2017_v2020.csv\"), index_col=\"Image Index\")\n",
    "ds[\"View Position\"] = datae[\"View Position\"]  # same index\n",
    "# display(ds)\n",
    "\n",
    "print(\"files read:\")\n",
    "print(f\"dataset: {ds.shape}\")\n",
    "print(f\"\\t - with columns: {ds.columns}\")\n",
    "# <\n",
    "\n",
    "# > ViewPosition and NoFinding are both in the index and in the columns\n",
    "def filter_columns(df):\n",
    "    # df = df.drop(columns=[\"View Position\", \"No Finding\"])  # after groupby the two columns are also in the index...\n",
    "    # df = df.reset_index().set_index(\"Image Index\")  # ...so reset_index() would fail if not dropped \n",
    "    df = df.drop(columns=[c for c in df.columns if (c not in paper_labels)])\n",
    "    return df\n",
    "# <\n",
    "\n",
    "\n",
    "# >\n",
    "def train_validation_split(dataset, n_train, n_valid, n_test, seed=1, n_iter=1):\n",
    "    #> single test set for all the train/valid splits\n",
    "    test = dataset[dataset.split == \"test\"]\n",
    "    test_set = test.groupby([\"No Finding\", \"View Position\"]).apply(lambda x: x.sample( n_test//4, random_state=shuffle_seed) )\n",
    "    test_set = test_set.reset_index(level=[0,1], drop=True)\n",
    "    test_set = filter_columns(test_set)\n",
    "\n",
    "    train_data = dataset[dataset.split == \"train\"]\n",
    "    print(f\"* all training examples: {train_data.shape}\")\n",
    "    print(train_data[\"No Finding\"].value_counts() / len(train_data))\n",
    "    print(train_data[\"View Position\"].value_counts() / len(train_data))\n",
    "    print(train_data[[\"No Finding\", \"View Position\"]].value_counts() / len(train_data))\n",
    "\n",
    "    for i in range(n_iter):        \n",
    "        training_set = train_data.groupby([\"No Finding\", \"View Position\"]).apply(lambda x: x.sample( (n_train + n_valid) // 4, random_state=seed+i) )\n",
    "        training_set, validation_set = train_test_split(training_set, \n",
    "                                                        test_size=1000, \n",
    "                                                        shuffle=True, \n",
    "                                                        random_state=shuffle_seed, \n",
    "                                                        stratify=training_set[[\"No Finding\", \"View Position\"]])\n",
    "        # print(\"training set:\", training_set.shape)\n",
    "        # print(\"validation set\", validation_set.shape)\n",
    "        # index for training_set and validation_set is: \"No FInding\", \"View Position\", \"Image Index\"\n",
    "        #   remove first two level\n",
    "        training_set = training_set.reset_index(level=[0,1], drop=True)\n",
    "        print(\"*TRAIN*\")\n",
    "        print(training_set.loc[:,[\"No Finding\", \"View Position\"]].value_counts() / len(training_set))\n",
    "        training_set = filter_columns(training_set)\n",
    "\n",
    "        validation_set = validation_set.reset_index(level=[0,1], drop=True)\n",
    "        print(\"*VALID*\")\n",
    "        print(validation_set.loc[:,[\"No Finding\", \"View Position\"]].value_counts() / len(validation_set))\n",
    "        validation_set = filter_columns(validation_set)\n",
    "        \n",
    "        yield training_set, validation_set, test_set\n",
    "    # <\n",
    "\n",
    "# > produce ecvl yaml\n",
    "def ecvl_yaml(filenames, labels, train_ids, valid_ids, test_ids):\n",
    "    d = {\n",
    "        \"name\"        : \"chest-xrays8, normal-vs-rest\",\n",
    "        \"description\" : \"normal-vs-rest\",\n",
    "        \"classes\"     : [], \n",
    "        \"images\"      : [],\n",
    "        \"split\"       : dict(training = train_ids, \n",
    "                            validation = valid_ids, \n",
    "                            test=test_ids)\n",
    "    }\n",
    "    imgs = []\n",
    "    for fn, l in zip(filenames, labels):\n",
    "        imgs.append({\n",
    "            \"location\": fn,\n",
    "            \"label\": l\n",
    "        })\n",
    "    d[\"images\"] = imgs\n",
    "    d[\"classes\"] = sorted(list(set(labels)))\n",
    "    return d\n",
    "# <\n",
    "\n",
    "def prepare_full_dataset(train, valid, test, label_col):\n",
    "    df = pd.concat([\n",
    "            (train.reset_index())[[\"Image Index\", label_col]],\n",
    "            (valid.reset_index())[[\"Image Index\", label_col]],\n",
    "            (test.reset_index())[[\"Image Index\", label_col]]\n",
    "    ], axis=0)\n",
    "    return df\n",
    "# <\n",
    "\n",
    "\n",
    "# > \n",
    "import os\n",
    "import yaml\n",
    "output_fld = \"/mnt/datasets/uc5/UC5_pipeline_forked/experiments_eddl/chestxray_normal\"\n",
    "\n",
    "for i, (training_set, validation_set, test_set) in \\\n",
    "    enumerate(train_validation_split(ds, 4000, 1000, 2000, seed=shuffle_seed, n_iter=3)):\n",
    "    # > check intersection\n",
    "    def common_index(d1, d2):\n",
    "        idx = d1.index.isin(d2.index)\n",
    "        return idx\n",
    "    #\n",
    "    assert nnz(common_index(training_set, validation_set))==0\n",
    "    assert nnz(common_index(training_set, test_set)) ==0 \n",
    "    assert nnz(common_index(validation_set, test_set)) == 0\n",
    "    # <\n",
    "\n",
    "    # prepare single dataframe with all the examples\n",
    "    examples = prepare_full_dataset(training_set, validation_set, test_set, \"No Finding\")\n",
    "    filenames = examples[\"Image Index\"].tolist()\n",
    "    labels = examples[\"No Finding\"].tolist()\n",
    "    # idxs for ecvl dataset\n",
    "    train_ids = list((range(len(training_set))))\n",
    "    validation_ids = [train_ids[-1] + v for v in list(range(len(validation_set)))]\n",
    "    test_ids = [validation_ids[-1] + v for v  in list(range(len(test_set)))]\n",
    "\n",
    "    ecvl_ds = ecvl_yaml(filenames, labels, train_ids, validation_ids, test_ids)\n",
    "    folder = join( output_fld, f\"fold_{i}\")\n",
    "    os.makedirs(folder, exist_ok = True)\n",
    "    with open(join(folder, \"dataset.yml\"), \"w\") as fout:\n",
    "        yaml.safe_dump(ecvl_ds, fout, default_flow_style=True)\n",
    "    \n",
    "    print(f\"dataset {examples.shape}, saved in: {folder}\")  \n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ds.groupby([\"No Finding\", \"View Position\"], as_index=False).apply(lambda x: x.sample( (5000) // 4, random_state=1) )\n",
    "display(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to ecvl dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data for the ecvl dataset\n",
    "\n",
    "def prepare_full_dataset(train, valid, test, label_col):\n",
    "    df = pd.concat([\n",
    "            (train.reset_index())[[\"Image Index\", label_col]],\n",
    "            (valid.reset_index())[[\"Image Index\", label_col]],\n",
    "            (test.reset_index())[[\"Image Index\", label_col]]\n",
    "    ], axis=0)\n",
    "    return df\n",
    "# <\n",
    "\n",
    "examples = prepare_full_dataset(training_set, validation_set, test_set, \"No Finding\").reset_index()\n",
    "print(ds.split.value_counts())\n",
    "display(examples)\n",
    "\n",
    "# > t e s t: PASSED\n",
    "# ex = examples.iloc[len(training_set)-1]\n",
    "# idx = ex[\"Image Index\"]\n",
    "# split = ds.loc[idx, \"split\"]\n",
    "# assert split == \"train\"\n",
    "\n",
    "# ex = examples.iloc[len(training_set)+1]\n",
    "# idx = ex[\"Image Index\"]\n",
    "# print(idx)\n",
    "# print(validation_set)\n",
    "# assert idx in validation_set.index\n",
    "\n",
    "# ex = examples.iloc[len(training_set) + len(validation_set) +1]\n",
    "# idx = ex[\"Image Index\"]\n",
    "# split = ds.loc[idx, \"split\"]\n",
    "# assert split == \"test\"\n",
    "# <\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# > produce ecvl yaml\n",
    "def ecvl_yaml(filenames, labels, train_ids, valid_ids, test_ids):\n",
    "    d = {\n",
    "        \"name\"        : \"chest-xrays8, normal-vs-rest\",\n",
    "        \"description\" : \"normal-vs-rest\",\n",
    "        \"classes\"     : [], \n",
    "        \"images\"      : [],\n",
    "        \"split\"       : dict(training = train_ids, \n",
    "                            validation = valid_ids, \n",
    "                            test=test_ids)\n",
    "    }\n",
    "    imgs = []\n",
    "    for fn, l in zip(filenames, labels):\n",
    "        imgs.append({\n",
    "            \"location\": fn,\n",
    "            \"label\": l\n",
    "        })\n",
    "    d[\"images\"] = imgs\n",
    "    d[\"classes\"] = sorted(list(set(labels)))\n",
    "    return d\n",
    "# <\n",
    "\n",
    "\n",
    "# > \n",
    "filenames = examples[\"Image Index\"].tolist()\n",
    "labels = examples[\"No Finding\"].tolist()\n",
    "train_ids = list((range(len(training_set))))\n",
    "validation_ids = [train_ids[-1] + v for v in list(range(len(validation_set)))]\n",
    "test_ids = [validation_ids[-1] + v for v  in list(range(len(test_set)))]\n",
    "\n",
    "ecvl_ds = ecvl_yaml(filenames, labels, train_ids, validation_ids, test_ids)\n",
    "\n",
    "import yaml\n",
    "output_fld = \"/mnt/datasets/uc5/UC5_pipeline_forked/experiments_eddl/chestxray_normal\"\n",
    "os.makedirs(output_fld, exist_ok=True)\n",
    "with open(join(output_fld, \"dataset.yml\"), \"w\") as fout:\n",
    "    yaml.safe_dump(ecvl_ds, fout, default_flow_style=True)\n",
    "\n",
    "print(f\"dataset saved in: {output_fld}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Following cells are tests or outdated</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study frequencies\n",
    "\n",
    "cols = [c for c in ds.columns if c != \"split\"]\n",
    "print(cols)\n",
    "train_ds = ds.loc[ds.split == \"train\", cols]\n",
    "test_ds  = ds.loc[ds.split == \"test\", cols]\n",
    "\n",
    "train_freqs = train_ds.sum(axis=0).to_numpy()\n",
    "test_freqs = test_ds.sum(axis=0)\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(121)\n",
    "\n",
    "_ = ax.bar(cols, train_freqs)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "_ = ax.bar(cols, test_freqs, alpha=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.index.name = \"filename\"\n",
    "test_ds.index.name = \"filename\"\n",
    "train_ds.to_csv( join(base_fld, \"train_set.csv\"))\n",
    "test_ds.to_csv( join(base_fld, \"test_set.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = pd.read_csv( join(base_fld, \"train_set.csv\"), index_col=\"filename\")\n",
    "test_ds = pd.read_csv( join(base_fld, \"test_set.csv\"), index_col=\"filename\")\n",
    "cols = train_ds.columns.to_numpy()\n",
    "\n",
    "def labels_to_str(row):\n",
    "    e = row.to_numpy().astype(bool)\n",
    "    return \";\".join(cols[e])\n",
    "\n",
    "train_ds[\"labels\"] = train_ds.apply(lambda row: labels_to_str(row), axis=1)\n",
    "label_counts = train_ds[\"labels\"].value_counts()\n",
    "print(label_counts)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8ac5ee7dbd3222b9a621db727279133c0b9b65990c5851472ddfcb34807bcee"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('eddl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
