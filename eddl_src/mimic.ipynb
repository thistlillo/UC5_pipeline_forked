{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIC-CXR (dcm & jpg)\n",
    "<font color=\"yellow\">0. IMPORT AND DATA</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from numpy import count_nonzero as nnz\n",
    "import os\n",
    "from posixpath import join\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "\n",
    "# ! random seed\n",
    "\n",
    "jpg_fld = \"/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0\"\n",
    "dcm_fld = \"/mnt/datasets/mimic-cxr/mimic-cxr-dcm/physionet.org/files/mimic-cxr/2.0.0\"\n",
    "\n",
    "# all the following files are split between the DCM and JPG versions\n",
    "\n",
    "filename_records = join(dcm_fld, \"cxr-record-list.csv\") # path to dmc images (jpg are basically mirrored)\n",
    "filename_reports = join(dcm_fld, \"cxr-study-list.csv\")  # text reports\n",
    "filename_metadata = join(jpg_fld, \"mimic-cxr-2.0.0-metadata.csv\") \n",
    "filename_chexpert = join(jpg_fld, \"mimic-cxr-2.0.0-chexpert.csv\") #labels\n",
    "filename_negbio = join(jpg_fld, \"mimic-cxr-2.0.0-negbio.csv\") #labels\n",
    "filename_split = join(jpg_fld, \"mimic-cxr-2.0.0-split.csv\") #suggested training-test split\n",
    "filename_sections = join(dcm_fld, \"mimic-cxr-sections\", \"mimic_cxr_sectioned.csv\")\n",
    "# preprocessing code from: https://github.com/MIT-LCP/mimic-cxr/tree/master/notebooks\n",
    "df = pd.read_csv(filename_records, header=0, sep=',')\n",
    "\n",
    "n = df.shape[0]\n",
    "print(f'{n} DICOMs in MIMIC-CXR v2.0.0.')\n",
    "\n",
    "n = df['study_id'].nunique()\n",
    "print(f'  {n} studies.')\n",
    "\n",
    "n = df['subject_id'].nunique()\n",
    "print(f'  {n} subjects.')\n",
    "\n",
    "dicoms = set(df['dicom_id'].tolist())\n",
    "\n",
    "df_split = pd.read_csv(filename_split)\n",
    "df_metadata = pd.read_csv(filename_metadata)\n",
    "\n",
    "display(df_metadata.T)\n",
    "\n",
    "cx = pd.read_csv(filename_chexpert)\n",
    "nb = pd.read_csv(filename_negbio)\n",
    "\n",
    "print(\"chexpert\")\n",
    "display(cx.T)\n",
    "print(\"negbio\")\n",
    "display(nb.T)\n",
    "\n",
    "cols = [col for col in cx.columns if col not in nb.columns]\n",
    "cols = cols + [col for col in nb.columns if col not in cx.columns]\n",
    "\n",
    "assert len(cols) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">1. VIEW</font> correct \"view\" field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize view with a mapping from ViewPosition\n",
    "VIEW_MAP = {\n",
    "    'AP': 'frontal',\n",
    "    'PA': 'frontal',\n",
    "    'LATERAL': 'lateral',\n",
    "    'LL': 'lateral',\n",
    "    'LPO': 'other',\n",
    "    'RAO': 'other',\n",
    "    'RPO': 'other',\n",
    "    'LAO': 'other',\n",
    "    # the below are overwritten in some instances by manual review\n",
    "    'AP AXIAL': 'other',\n",
    "    'XTABLE LATERAL': 'other',\n",
    "    'AP LLD': 'other',\n",
    "    'PA LLD': 'other',\n",
    "    'L5 S1': 'other',\n",
    "    'SWIMMERS': 'other',\n",
    "    'AP RLD': 'other',\n",
    "    'PA RLD': 'other',\n",
    "}\n",
    "\n",
    "df_metadata['view'] = df_metadata['ViewPosition'].map(VIEW_MAP)\n",
    "\n",
    "# for 'other' category, currently many of these are simply unknown\n",
    "# so try to update them with acq device map\n",
    "ADPD_MAP = {\n",
    "    'CHEST, LATERAL': 'lateral',\n",
    "    'CHEST, PA': 'frontal',\n",
    "    # manually checked 100 records, below is always frontal\n",
    "    'CHEST, PORTABLE': 'frontal',\n",
    "    'CHEST, PA X-WISE': 'frontal',\n",
    "    'CHEST, AP (GRID)': 'frontal',\n",
    "    'CHEST LAT': 'lateral',\n",
    "    'CHEST PA': 'frontal',\n",
    "    'CHEST, AP NON-GRID': 'frontal',\n",
    "    'CHEST AP NON GRID': 'frontal',\n",
    "    'CHEST PA X-WISE': 'frontal',\n",
    "    'CHEST AP GRID': 'frontal',\n",
    "    'CHEST, PORTABLE X-WISE': 'other',\n",
    "    # below have < 25 samples each\n",
    "    'CHEST PORT': 'frontal',\n",
    "    'CHEST PORT X-WISE': 'frontal',\n",
    "    # manually classified below\n",
    "    'SHOULDER': 'other',\n",
    "    'CHEST, PEDI (4-10 YRS)': 'other',\n",
    "    'LOWER RIBS': 'other',\n",
    "    'CHEST, DECUB.': 'other',\n",
    "    'ABDOMEN, PORTABLE': 'other',\n",
    "    'UPPER RIBS': 'frontal',\n",
    "    'STERNUM, LATERAL': 'lateral',\n",
    "    'KNEE, AP/OBL': 'other',\n",
    "    'STERNUM, PA/OBL.': 'other',\n",
    "    'CLAVICLE/ AC JOINTS': 'other',\n",
    "    'ABDOMEN,GENERAL': 'other',\n",
    "    'LOWER RIB': 'other',\n",
    "    'SCOLIOSIS AP': 'frontal'\n",
    "}\n",
    "\n",
    "good_view = ['frontal', 'lateral']\n",
    "idxUpdate = ~df_metadata['view'].isin(good_view)\n",
    "\n",
    "# ! this field is not present in current file\n",
    "c = 'AcquisitionDeviceProcessingDescription'\n",
    "# idx = (df_metadata[c].notnull()) & idxUpdate\n",
    "# df_metadata.loc[idx, 'view'] = df_metadata.loc[idx, c].map(ADPD_MAP)\n",
    "\n",
    "DICOM_TO_VIEW = {\n",
    "    '2164992c-f4abb30a-7aaaf4f4-383cab47-4e3eb1c8': ['PA', 'frontal'],\n",
    "    '5e6881e2-ff4254e0-b99f0c2f-8964482a-031364db': ['LL', 'lateral'],\n",
    "    'fcdf7a30-3236b74e-65b97587-cdd4cfde-63cd1de0': ['PA', 'frontal'],\n",
    "    'fb074ec1-6715839c-84fa75e6-adc3f026-448b1481': ['PA', 'frontal'],\n",
    "    'dfb8080a-8506e43e-840d9d58-0f738f41-82c120b0': ['PA', 'frontal'],\n",
    "    '4b32608b-c2ead7c4-1fe5565f-42f7ab80-9dad30de': ['LL', 'lateral'],\n",
    "    '53663e89-8f9ca9bb-df1bf434-8d6b1283-2b612609': ['LL', 'lateral'],\n",
    "    # below are AP, but incorrectly in View Position\n",
    "    '8672a4e7-366801a0-26cf2395-9344335c-aac8d728': ['AP', 'frontal'],\n",
    "    '9800b28e-3ff3b417-18473be2-1a66131d-aca88488': ['AP', 'frontal'],\n",
    "    '598cfe48-33a8643e-843e27e2-5dd584e7-3cd5f1c0': ['AP', 'frontal']\n",
    "}\n",
    "\n",
    "# we manually reviewed a few DICOMs to keep them in\n",
    "for dcm, row in DICOM_TO_VIEW.items():\n",
    "    view = row[1]\n",
    "    idx = df_metadata['dicom_id'] == dcm\n",
    "    if idx.any():\n",
    "        df_metadata.loc[idx, 'view'] = view\n",
    "\n",
    "# remove rows that do not have a good view\n",
    "idxUpdate = ~df_metadata['view'].isin(good_view)\n",
    "print(f\"removing {nnz(idxUpdate)} rows\")\n",
    "n_rows = df_metadata.shape[0]\n",
    "df_metadata.drop(df[idxUpdate].index, inplace=True)\n",
    "assert nnz(idxUpdate) == n_rows - df_metadata.shape[0]\n",
    "\n",
    "print(f\"using {df_metadata.shape[0]} examples\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">2. SPLIT & FINDINGS</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes\n",
    "# METADATA + SPLIT\n",
    "df = df_split.merge(df_metadata.drop(['study_id', 'subject_id'], axis=1),\n",
    "                   on='dicom_id', how='inner')\n",
    "\n",
    "nb = pd.read_csv(filename_negbio)\n",
    "cx = pd.read_csv(filename_chexpert)\n",
    "\n",
    "nb.drop('subject_id', axis=1, inplace=True)\n",
    "cx.drop('subject_id', axis=1, inplace=True)\n",
    "\n",
    "nb_findings = [x for x in nb.columns if x != 'study_id']\n",
    "cx_findings = [x for x in cx.columns if x != 'study_id']\n",
    "\n",
    "# METADATA + SPLIT + LABELS\n",
    "df = df.merge(cx, how='left', on='study_id')\n",
    "\n",
    "# indicator flag for the study having a Chexpert/NegBio finding\n",
    "df['has_chexpert_finding'] = df[[x for x in cx_findings if x != 'No Finding']].notnull().sum(axis=1) > 0\n",
    "df['has_chexpert_pos_finding'] = df[[x for x in cx_findings if x != 'No Finding']].gt(0).sum(axis=1) > 0\n",
    "df['is_chexpert_normal'] = df[['No Finding']].gt(0).sum(axis=1) > 0\n",
    "df['has_chexpert_neg_finding'] = df[[x for x in cx_findings if x != 'No Finding']].lt(0).sum(axis=1) > 0\n",
    "# df['has_chexpert_pos_neg'] = df[[x for x in cx_findings if x != 'No Finding']].lt(0).sum(axis=1) > 0 & df[[x for x in cx_findings if x != 'No Finding']].gt(0).sum(axis=1) > 0\n",
    "# df['has_negbio_finding'] = df[[x for x in nb_findings if x != 'No Finding']].notnull().sum(axis=1) > 0\n",
    "# df['has_negbio_pos_finding'] = df[[x for x in nb_findings if x != 'No Finding']].notnull().sum(axis=1) > 0\n",
    "\n",
    "df.drop(df[~df.view.isin([\"frontal\", \"lateral\"])].index, inplace=True)\n",
    "print(\"view\")\n",
    "print(df[\"view\"].value_counts())\n",
    "\n",
    "# df[['dicom_id', 'split', 'view'] + findings].head().T\n",
    "\n",
    "print(\"metadata + split + chexpert:\", df.shape)\n",
    "display(df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 use the entire training set\n",
    "\n",
    "target = \"No Finding\"\n",
    "\n",
    "df[\"target\"] = df[target]\n",
    "df.loc[df[\"target\"].isna(), \"target\"] = 0  # No Finding is 1\n",
    "\n",
    "display(df[['dicom_id', 'view']].groupby(['view']).count() / len(df))\n",
    "display(df[['dicom_id', 'target']].groupby(['target']).count() / len(df))\n",
    "\n",
    "sub_columns = [\"study_id\", \"dicom_id\", \"view\", \"target\", \"split\"]\n",
    "training_set = df.loc[df.split==\"train\", sub_columns]  # .groupby([\"study_id\", \"target\"]).sample(1)\n",
    "validation_set = df.loc[df.split==\"validate\", sub_columns]\n",
    "test_set = df.loc[df.split == \"test\", sub_columns]\n",
    "\n",
    "print(f\"* training set {len(training_set)}\")\n",
    "print(  training_set[[\"dicom_id\", \"target\"]].groupby(  [\"target\"]  ).count() / len(training_set)  )\n",
    "\n",
    "print(f\"* validation set {len(validation_set)}\")\n",
    "print(  validation_set[[\"dicom_id\", \"target\"]].groupby(  [\"target\"]  ).count() / len(validation_set)  )\n",
    "\n",
    "print(f\"* test set {len(test_set)}\")\n",
    "print(  test_set[[\"dicom_id\", \"target\"]].groupby(  [\"target\"]  ).count() / len(test_set)  )\n",
    "\n",
    "sub_columns = [\"dicom_id\", \"target\", \"split\"]\n",
    "dataset = pd.concat(  [training_set[sub_columns], validation_set[sub_columns], test_set[sub_columns]], axis=0  )\n",
    "print(dataset.shape)\n",
    "\n",
    "paths = pd.read_csv(filename_records)\n",
    "display(paths)\n",
    "\n",
    "dataset = dataset.merge(paths[[\"dicom_id\", \"path\"]], on=\"dicom_id\", how=\"left\")\n",
    "display(dataset.T)\n",
    "\n",
    "out_fld = \"/mnt/datasets/mimic-cxr/training_data/mimic\"\n",
    "dataset.to_csv( join(out_fld, \"normal_bin_unbal.tsv\"), sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">unbalanced training sets</font>\n",
    "50%-50% normal vs rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No-finding vs rest\n",
    "# encoding no_finding = 1, with_findings = 1\n",
    "# 1 image per dicom study: either training or validation study\n",
    "# test set untouched (so not balanced)\n",
    "\n",
    "# we now have one image per available view for each study\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "values = [[np.nan, 1], [np.nan, 1], [1, 1], [np.nan, 1], [3, 2]]\n",
    "df = pd.DataFrame(data=values, columns=[\"A\", \"B\"])\n",
    "print(df)\n",
    "df[\"A\"].where(~df[\"A\"].isna(), 0, inplace=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set\n",
    "splits = [\"validate\", \"test\"]\n",
    "d = df.loc[df.split.isin(splits), [\"split\", \"dicom_id\", \"study_id\", \"view\"]]\n",
    "d = d.merge(cx, on=\"study_id\", how=\"left\")\n",
    "for p in splits:\n",
    "    partitions[p] = d[d.split==p].where(~d[\"No Finding\"].isna(), 0)\n",
    "for p in partitions:\n",
    "    print(p)\n",
    "    display(partitions[p].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels for chexpert\n",
    "display(d)\n",
    "display(cx)\n",
    "display(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(d.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">STATS ABOUT HAS_FINDING</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'validate', 'test']\n",
    "split_views = df.groupby(['split', 'view'])[['dicom_id']].count()\n",
    "row_idx = ['frontal', 'lateral']  #, 'other'] other removed\n",
    "\n",
    "# number of images in each set\n",
    "n_images = {}\n",
    "for c in splits:\n",
    "    n_images[c] = split_views.loc[c].loc[row_idx, 'dicom_id'].sum()\n",
    "\n",
    "tbl = pd.DataFrame.from_dict(n_images, orient='index')\n",
    "tbl.columns = ['Number of images']\n",
    "tbl = tbl.T\n",
    "display(tbl)\n",
    "\n",
    "# number of images in each set for each view\n",
    "n_images = {}\n",
    "for c in splits:\n",
    "    n_images[c] = {}\n",
    "    for view in row_idx:\n",
    "        n_images[c][view] = split_views.loc[c].loc[view, 'dicom_id']\n",
    "n_images = pd.DataFrame.from_dict(n_images, orient='index')\n",
    "display(n_images)\n",
    "n_images = n_images.T\n",
    "display(n_images)\n",
    "\n",
    "# convert frontal/lateral/other into \"N (%)\"\n",
    "for i in n_images.index:\n",
    "    for c in splits:\n",
    "        val = n_images.loc[i, c]\n",
    "        n_images.loc[i, c] = f'{val} ({100.0*val/tbl.loc[\"Number of images\", c]:3.1f}%)'\n",
    "\n",
    "tbl = pd.concat([tbl, n_images], axis=0, sort=False)\n",
    "\n",
    "# add in the number of subjects\n",
    "n_studies = df.groupby('split')[['study_id']].nunique().T\n",
    "n_studies.index = ['Number of studies']\n",
    "tbl = pd.concat([tbl, n_studies], axis=0, sort=False)\n",
    "\n",
    "# studies with a finding\n",
    "n_studies = df.loc[df['has_chexpert_finding']].groupby('split')[['study_id']].nunique().T\n",
    "n_studies.index = ['  with a finding']\n",
    "for c in splits:\n",
    "    val = n_studies.loc['  with a finding', c]\n",
    "    n_studies.loc['  with a finding', c] = f'{val} ({100.0*val/tbl.loc[\"Number of studies\", c]:3.1f}%)'\n",
    "tbl = pd.concat([tbl, n_studies], axis=0, sort=False)\n",
    "\n",
    "# patients\n",
    "n_pt = df.groupby('split')[['subject_id']].nunique().T\n",
    "n_pt.index = ['Number of patients']\n",
    "tbl = pd.concat([tbl, n_pt], axis=0, sort=False)\n",
    "\n",
    "# patients with a finding\n",
    "n_studies = df.loc[df['has_chexpert_finding']].groupby('split')[['subject_id']].nunique().T\n",
    "n_studies.index = ['  with a finding']\n",
    "for c in splits:\n",
    "    val = n_studies.loc['  with a finding', c]\n",
    "    n_studies.loc['  with a finding', c] = f'{val} ({100.0*val/tbl.loc[\"Number of patients\", c]:3.1f}%)'\n",
    "tbl = pd.concat([tbl, n_studies], axis=0, sort=False)\n",
    "\n",
    "# tbl.to_latex('table2.tex')\n",
    "\n",
    "display(tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">AGREEMENT</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = pd.read_csv(filename_negbio)\n",
    "cx = pd.read_csv(filename_chexpert)\n",
    "\n",
    "# merge these findings to create a table\n",
    "# both agree -> output label\n",
    "# disagree -> output -9\n",
    "\n",
    "# drop subject_id from cx - we have it in nb\n",
    "df = cx.merge(\n",
    "    nb.drop('subject_id', axis=1),\n",
    "    how='left',\n",
    "    left_on='study_id', right_on='study_id',\n",
    "    suffixes=('', '_nb')\n",
    ")\n",
    "\n",
    "# subselect to training set\n",
    "study_ids = set(df_split.loc[df_split['split']=='train', 'study_id'])\n",
    "df = df.loc[df['study_id'].isin(study_ids)]\n",
    "\n",
    "# replace numeric labels with meaningful labels\n",
    "# also annotate disagreements between the two labelers\n",
    "labels = {0: 'Negative', 1: 'Positive', -1: 'Uncertain', -9: 'Disagreement'}\n",
    "for c in df.columns:\n",
    "    if c in ('subject_id', 'study_id'):\n",
    "        continue\n",
    "    elif c.endswith('_nb'):\n",
    "        continue\n",
    "    \n",
    "    # chexpert column\n",
    "    c_nb = f'{c}_nb'\n",
    "    \n",
    "    # annotate disagreement\n",
    "    for val in labels.keys():\n",
    "        if val == -9:\n",
    "            continue\n",
    "        \n",
    "        # check one is null and the other isn't\n",
    "        idx = df[c].isnull() & df[c_nb].notnull()\n",
    "        df.loc[idx, c] = -9\n",
    "        \n",
    "        idx = df[c].notnull() & df[c_nb].isnull()\n",
    "        df.loc[idx, c] = -9\n",
    "        \n",
    "        # check both non-null, but different value\n",
    "        idx = df[c].notnull() & df[c_nb].notnull() & (df[c] != df[c_nb])\n",
    "        df.loc[idx, c] = -9\n",
    "        \n",
    "    # now for those missing in chexpert\n",
    "    idx = df[c].isnull() & df[f'{c}_nb'].notnull()\n",
    "    df.loc[idx, c] = -9\n",
    "    \n",
    "    df[c] = df[c].map(labels)\n",
    "    \n",
    "# drop chexpert columns\n",
    "cols_drop = [c for c in df.columns if c.endswith('_nb')]\n",
    "df.drop(cols_drop, axis=1, inplace=True)\n",
    "\n",
    "# display a few example cases\n",
    "display(df.head(n=10))\n",
    "\n",
    "# create a summary table of the findings\n",
    "grp_cols = [c for c in df.columns if c not in ('subject_id', 'study_id')]\n",
    "tbl = {}\n",
    "for c in grp_cols:\n",
    "    tbl[c] = df[c].value_counts().to_dict()\n",
    "tbl = pd.DataFrame.from_dict(tbl, orient='index')\n",
    "\n",
    "\n",
    "# pretty format the labels\n",
    "N = df.shape[0]\n",
    "for c in tbl.columns:\n",
    "    tbl[c] = tbl[c].apply(lambda x: f'{x:,} ({100.0*x/N:3.1f}%)')\n",
    "\n",
    "# sort columns\n",
    "print(f'Frequency of labels in MIMIC-CXR-JPG on the training subset of {df.shape[0]:,} unique radiologic studies.')\n",
    "tbl = tbl[['Positive', 'Negative', 'Uncertain', 'Disagreement']]\n",
    "# tbl.to_latex('findings_frequency.tex')\n",
    "tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename_records, header=0, sep=',')\n",
    "\n",
    "sections = pd.read_csv(filename_sections)\n",
    "\n",
    "print(sections.columns)\n",
    "N = df['study_id'].nunique()\n",
    "print(f'Of the total {N:,} reports.. ')\n",
    "idx = sections['study'].notnull()\n",
    "for c in ['impression', 'findings', 'last_paragraph']:\n",
    "    n = sections.loc[idx, c].count()\n",
    "    print(f'  {n:,} ({100.0*n/N:3.1f}%) had a {c} section')\n",
    "    # limit next check to only rows where this section is null\n",
    "    idx = idx & sections[c].isnull()\n",
    "\n",
    "\n",
    "\n",
    "# ! code from github ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">my code</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dcm: read all files\n",
    "ds = pd.read_csv(filename_records).set_index([\"subject_id\", \"study_id\"])\n",
    "print(\"record list - from dcm, shape:\", ds.shape)\n",
    "display(ds)\n",
    "\n",
    "# from jpg\n",
    "metadata = pd.read_csv( join(jpg_fld, \"mimic-cxr-2.0.0-metadata.csv\") ).set_index([\"subject_id\", \"study_id\"])\n",
    "print(\"metadata - from jpg, shape:\", metadata.shape)\n",
    "display(metadata)\n",
    "\n",
    "\n",
    "# jpg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = metadata.ViewPosition\n",
    "print(vp.value_counts())\n",
    "ds[\"ViewPosition\"] = vp\n",
    "\n",
    "display(ds)\n",
    "\n",
    "# print(\"PA\")\n",
    "# pa = ds.loc[ds.ViewPosition == \"PA\", [\"path\"]]\n",
    "# fns = pa.iloc[[0,1,2]].path\n",
    "# for fn in fns:\n",
    "#     fn = fn.replace(\".dcm\", \".jpg\")\n",
    "#     display(Image( join(jpg_fld, fn), width=224, height=224))\n",
    "\n",
    "# print(\"AP\")\n",
    "# pa = ds.loc[ds.ViewPosition == \"AP\", [\"path\"]]\n",
    "# fns = pa.iloc[[0,1,2]].path\n",
    "# for fn in fns:\n",
    "#     fn = fn.replace(\".dcm\", \".jpg\")\n",
    "#     display(Image( join(jpg_fld, fn), width=224, height=224))\n",
    "\n",
    "# print(\"LATERAL\")\n",
    "# pa = ds.loc[ds.ViewPosition == \"LATERAL\", [\"path\"]]\n",
    "# fns = pa.iloc[[0,1,2]].path\n",
    "# for fn in fns:\n",
    "#     fn = fn.replace(\".dcm\", \".jpg\")\n",
    "#     display(Image( join(jpg_fld, fn), width=224, height=224))\n",
    "\n",
    "# print(\"XTABLE LATERAL\")\n",
    "# pa = ds.loc[ds.ViewPosition == \"XTABLE LATERAL\", [\"path\"]]\n",
    "# print(len(pa))\n",
    "# fns = pa.iloc[[0,1]].path\n",
    "# for fn in fns:\n",
    "#     fn = fn.replace(\".dcm\", \".jpg\")\n",
    "#     display(Image( join(jpg_fld, fn), width=224, height=224))\n",
    "\n",
    "print(\"LL\")\n",
    "pa = ds.loc[ds.ViewPosition == \"LL\", [\"path\"]]\n",
    "print(len(pa))\n",
    "fns = pa.iloc[[10,20]].path\n",
    "for fn in fns:\n",
    "    fn = fn.replace(\".dcm\", \".jpg\")\n",
    "    display(Image( join(jpg_fld, fn), width=224, height=224))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add column with labels\n",
    "- add column with text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read labels from chexpert/negbio in jpg folder\n",
    "for each label\n",
    "- +1 means positive mention\n",
    "- -1 means negative mention\n",
    "- 0 means uncertainty in the mention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jpg\n",
    "chexpert = pd.read_csv( join(jpg_fld, \"mimic-cxr-2.0.0-chexpert.csv\"), na_filter=False).set_index([\"subject_id\", \"study_id\"])\n",
    "print(\"chexpert - from jpg, shape:\", chexpert.shape)\n",
    "display(chexpert)\n",
    "\n",
    "columns = chexpert.columns\n",
    "print(f\"n labels: {len(columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several strategies for managing the uncertain label (zero), as suggested by the authors themselves:\n",
    "- U-Ignore: We ignore the uncertain labels during training.\n",
    "- U-Zeroes: We map all instances of the uncertain label to 0.\n",
    "- U-Ones: We map all instances of the uncertain label to 1.\n",
    "- U-SelfTrained: We first train a model using the U-Ignore approach to convergence, and then use the model to make predictions that re-label each of the uncertainty labels with the probability prediction outputted by the model.\n",
    "- U-MultiClass: We treat the uncertainty label as its own class.\n",
    "\n",
    "U-Ignore selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8ac5ee7dbd3222b9a621db727279133c0b9b65990c5851472ddfcb34807bcee"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('eddl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
