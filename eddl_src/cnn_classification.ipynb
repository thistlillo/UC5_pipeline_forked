{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from numpy import count_nonzero as nnz\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from posixpath import join\n",
    "import pyeddl.eddl as eddl\n",
    "import pyecvl.ecvl as ecvl\n",
    "from pyeddl.tensor import Tensor\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, jaccard_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#> paths\n",
    "fld = \"/mnt/datasets/uc5/UC5_pipeline_forked/experiments_eddl/eddl_ext_CNN_20tags\"\n",
    "img_fld = \"../data/image\"\n",
    "mdl_fn = join(fld, \"cnn_checkpoint.onnx\")\n",
    "ds_fn = join(fld, \"img_reports_ext_enc.tsv\")\n",
    "\n",
    "\n",
    "exs = {}\n",
    "partitions = {\"training\": \"train_ids.txt\", \"validation\": \"valid_ids.txt\", \"test\": \"test_ids.txt\"}\n",
    "for key, fn in partitions.items():\n",
    "    with open(join(fld, fn), \"r\") as fin:\n",
    "        lines = [line.strip() for line in fin.readlines()]\n",
    "        exs[key] = lines\n",
    "#<\n",
    "\n",
    "\n",
    "\n",
    "#>\n",
    "# ! gpu\n",
    "bs = 32\n",
    "\n",
    "mean = [0.48197903, 0.48197903, 0.48197903]\n",
    "std = [0.26261734, 0.26261734, 0.26261734]\n",
    "test_augs =  lambda x: ecvl.SequentialAugmentationContainer([\n",
    "                ecvl.AugResizeDim([300, 300]),\n",
    "                ecvl.AugCenterCrop([x, x]),\n",
    "                # ecvl.AugRandomCrop([size, size]),  # XXX should be parametric, for resnet 18\n",
    "                ecvl.AugToFloat32(divisor=255.0),\n",
    "                ecvl.AugNormalize(mean, std),\n",
    "            ])\n",
    "# ! image size\n",
    "test_augs = test_augs(224)\n",
    "#<\n",
    "\n",
    "#>\n",
    "cnn = eddl.import_net_from_onnx_file(mdl_fn)\n",
    "eddl_cs = eddl.CS_GPU(g=[1,0,0,0], mem=\"full_mem\")\n",
    "eddl.build(cnn, eddl.adam(0.01), [\"softmax_cross_entropy\"], [\"accuracy\"], eddl_cs, init_weights=False)\n",
    "eddl.set_mode(cnn, 0)\n",
    "#<\n",
    "\n",
    "ds = pd.read_csv(ds_fn, sep=\"\\t\")\n",
    "\n",
    "# ! careful here\n",
    "if False:\n",
    "    print(\"reducing dataset!\")\n",
    "    ds = ds.iloc[:100]\n",
    "\n",
    "def load_image(path, augs=None):\n",
    "    img = ecvl.ImRead(path)\n",
    "    if augs:\n",
    "        augs.Apply(img)\n",
    "    ecvl.RearrangeChannels(img, img, \"cxy\")\n",
    "    return img    \n",
    "\n",
    "def classify(img, cnn, theta = 0.5, dev=False):\n",
    "    cnn_out = eddl.getLayer(cnn, \"cnn_out\")\n",
    "    cnn_top = eddl.getLayer(cnn, \"top\")\n",
    "    # -\n",
    "    a = np.expand_dims(np.array(img, copy=False), axis=0)  # add batch dimension\n",
    "    eddl.forward(cnn, [Tensor.fromarray(a)])\n",
    "    # - \n",
    "    cnn_semantic = eddl.getOutput(cnn_out)\n",
    "    output = np.squeeze(np.array(cnn_semantic))\n",
    "    tags = np.where(output > theta)[0]\n",
    "    # c = np.argmax(classes, axis=-1)\n",
    "    return tags, output\n",
    "#<\n",
    "\n",
    "\n",
    "\n",
    "for key, l in exs.items():\n",
    "    tags = []\n",
    "    output = defaultdict(list)\n",
    "    \n",
    "    print(f\"split: {key}\")\n",
    "    indexes   = ds.filename.isin(l)\n",
    "    filenames = ds.loc[indexes, \"filename\"].tolist()\n",
    "    labels    = ds.loc[indexes, \"labels\"].tolist()\n",
    "    for fn in tqdm(filenames):\n",
    "        img  = load_image(join(img_fld, fn), augs=test_augs)\n",
    "        tags_, outs_ = classify(img, cnn)\n",
    "        tags.append(\";\".join(str(t) for t in tags_))\n",
    "        output[key].append(outs_)\n",
    "    ds.loc[indexes, \"split\"] = key\n",
    "    ds.loc[indexes, \"tags\"]  = tags\n",
    "\n",
    "ds.to_csv(join(fld, \"cnn_tags.tsv\"), sep=\"\\t\")\n",
    "with open(join(fld, \"output.pkl\"), \"wb\") as fout:\n",
    "    pickle.dump(output, fout)\n",
    "\n",
    "print(\"file saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.labels)\n",
    "print(ds.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ds = pd.read_csv(join(fld, \"cnn_tags.tsv\"), sep=\"\\t\", na_filter=False)\n",
    "with open(join(fld, \"output.pkl\"), \"rb\") as fin:\n",
    "    output = pickle.load(fin)\n",
    "\n",
    "with open( join(fld, \"index2lab.json\"), \"r\") as fin:\n",
    "    index2lab = json.load(fin)\n",
    "\n",
    "n_classes = len(index2lab)\n",
    "\n",
    "print(f\"|labels|={n_classes}\")\n",
    "print(ds.columns)\n",
    "\n",
    "def onehot(x):\n",
    "    global empty_cnt\n",
    "    if len(x) > 0:\n",
    "        values = [int(l) for l in x.split(\";\")]\n",
    "    else:\n",
    "        values = []\n",
    "    e = np.zeros((n_classes,), dtype=int)\n",
    "    e[values] = 1\n",
    "    return e\n",
    "\n",
    "target = ds.labels.apply(lambda x: onehot(x))\n",
    "pred = ds.tags.apply(lambda x: onehot(x))\n",
    "untagged_idx = ds.tags.str.len() == 0\n",
    "print(f\"|untagged exs|={nnz(untagged_idx)}\")\n",
    "\n",
    "\n",
    "\n",
    "splits = [\"training\", \"validation\", \"test\"]\n",
    "print(ds.split.unique())\n",
    "for s in splits:\n",
    "    idxs = ds.split.str.match(s)\n",
    "    print(f\"Split {s}, {nnz(idxs)}\")\n",
    "    target_values = ds.labels[idxs]\n",
    "    predicted_values = ds.tags[idxs]\n",
    "    if s == \"test\":\n",
    "        print(ds.loc[idxs, [\"labels\", \"tags\"]])\n",
    "\n",
    "    accuracy = accuracy_score(target_values, predicted_values)\n",
    "    jaccard = jaccard_score(target_values, predicted_values, labels=list(range(n_classes)), average=\"micro\")\n",
    "    print(f\"{s}, accuracy = {accuracy:.2f}\")\n",
    "    print(f\"{s}, jaccard = {jaccard:.2f}\")\n",
    "\n",
    "print(index2lab[\"16\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((10,))\n",
    "a[1] = 1\n",
    "a[3] = 2\n",
    "print(np.where(a > 0.5))\n",
    "print(a > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([ [1, 0, 0], [0, 1, 1] ])\n",
    "p = np.array([ [1, 0, 0], [0, 0, 1] ])\n",
    "\n",
    "print(jaccard_score(t, p, labels=[0,1,2], average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8ac5ee7dbd3222b9a621db727279133c0b9b65990c5851472ddfcb34807bcee"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('eddl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
