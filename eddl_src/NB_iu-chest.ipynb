{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from posixpath import join\n",
    "import numpy as np\n",
    "import random\n",
    "from numpy import count_nonzero as nnz\n",
    "from collections import defaultdict\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    " \n",
    "# note: ds_home is the base folder for the images\n",
    "ds_home = \"/mnt/datasets/uc5/std-dataset/image\"\n",
    "\n",
    "# some preprocessed files\n",
    "base_path = \"/mnt/datasets/uc5/UC5_pipeline_forked/experiments_eddl/mnt/datasets/uc5/UC5_pipeline_forked/experiments_eddl/eddl_ext_CNN_20tags\"\n",
    "# raw reports processed\n",
    "ds = pd.read_csv( \"/mnt/datasets/uc5/UC5_pipeline_forked/experiments_eddl/reports_raw.tsv\", sep=\"\\t\", na_filter=False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reports without images\n",
    "\n",
    "iii = ds.n_images == 0\n",
    "print(f\"*** number of reports without images {nnz(iii)}, removed\")\n",
    "ds = ds.loc[~iii].reset_index()\n",
    "\n",
    "num_reports = ds.shape[0]\n",
    "num_images = ds.n_images.sum()\n",
    "print(f\"reports {num_reports}, images {num_images}\")\n",
    "\n",
    "\n",
    "img_g = ds.loc[:, [\"id\", \"n_images\"]].groupby([\"n_images\"]).agg([\"count\"])\n",
    "display(img_g)\n",
    "\n",
    "mm_g = ds[[\"id\", \"n_major_mesh\"]].groupby([\"n_major_mesh\"]).agg([\"count\"])\n",
    "display(mm_g)\n",
    "\n",
    "at_g = ds[[\"id\", \"n_auto_term\"]].groupby([\"n_auto_term\"]).agg([\"count\"])\n",
    "display(at_g)\n",
    "\n",
    "normal_ids = ds.major_mesh == \"normal\"\n",
    "print(f\"normal reports (mesh): {nnz(normal_ids)}\")\n",
    "\n",
    "empty_auto_ids = ds.n_auto_term == 0\n",
    "normal_auto_ids = empty_auto_ids & normal_ids\n",
    "print(f\"empty auto terms that are normal according to mesh: {nnz(normal_auto_ids)}/{nnz(empty_auto_ids)}\")\n",
    "\n",
    "empty_auto_ids = empty_auto_ids & ~normal_auto_ids\n",
    "print(f\"empty auto terms that are not normal according to mesh: {nnz(empty_auto_ids)}\")\n",
    "\n",
    "print(\"major mesh, sample:\\n\\t\", random.sample(ds.major_mesh.tolist(), 5))\n",
    "print(\"auto term, sample:\\n\\t\", random.sample(ds.auto_term.tolist(), 5))\n",
    "\n",
    "def get_unique_set(col):\n",
    "    # out = [t.lower() for g in col for t in g.split(\";\") if len(g) > 0]\n",
    "    out = set()\n",
    "    for terms in col:\n",
    "        if len(terms) == 0:\n",
    "            continue\n",
    "        for t in terms.split(\";\"):\n",
    "            out.add(t.strip().lower())\n",
    "    return out\n",
    "\n",
    "u_mesh_combos = get_unique_set(ds.major_mesh)\n",
    "u_auto_combos = get_unique_set(ds.auto_term)\n",
    "\n",
    "print(f\"unique mesh combos: {len(u_mesh_combos)}\")\n",
    "print(f\"unique auto combos: {len(u_auto_combos)}\")\n",
    "\n",
    "print(\"unique mesh, sample:\", random.sample(u_mesh_combos, 10))\n",
    "print(\"unique auto, sample:\", random.sample(u_auto_combos, 10))\n",
    "\n",
    "mesh_in_empty_auto = get_unique_set(ds.major_mesh.loc[empty_auto_ids])\n",
    "print(f\"auto tags associated to non-normal empty auto terms: {len(mesh_in_empty_auto)} terms\")\n",
    "# print(mesh_in_empty_auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">MeSH terms only</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are no empty major mesh fields\n",
    "# expected syntax:\n",
    "import re\n",
    "print(\"sample of mesh terms\")\n",
    "print(random.sample(ds.major_mesh.tolist(), 3))\n",
    "\n",
    "# TODO\n",
    "# some headings terms appear more than once, ex:\n",
    "#      Calcified Granuloma/mediastinum/large;Calcified Granuloma/lung/hilum/right/large --> ['calcified granuloma', 'calcified granuloma']\n",
    "def simplify_terms(terms):\n",
    "    # split on ; [heading/subheadings, heading/subheading]\n",
    "    # then take only the first\n",
    "    out = [g.split(\"/\")[0].strip().lower() for g in terms.split(\";\")]  # heading/subheadings ; heading/subheadings ; \n",
    "    \n",
    "    # in some multi-word headings, words are separated by multiple spaces:\n",
    "    return list(set( [re.sub(r\"\\s+\", ' ', terms) for terms in out] ))\n",
    "\n",
    "simplified_mesh = ds.major_mesh.apply(lambda x: simplify_terms(x))\n",
    "# for m, s in zip(ds.major_mesh.tolist(), simplified_mesh.tolist()):\n",
    "#      print(f\" {m} --> {s}\")\n",
    "\n",
    "ds[\"labels\"] = simplified_mesh\n",
    "# labels_s: labels as string\n",
    "ds[\"labels_s\"] = simplified_mesh.apply(lambda x: \";\".join(x))  # labels joined as string\n",
    "ds[\"n_labels\"] = ds.labels.apply(lambda l: len(l))\n",
    "\n",
    "\n",
    "# the two \"counts\" that follow show different results because\n",
    "# some tags contain the same heading more than once followed by different subheadings (n_major_mesh)\n",
    "# in n_labels only unique headings are included\n",
    "mm_g = ds[[\"id\", \"n_major_mesh\"]].groupby([\"n_major_mesh\"]).agg([\"count\"])\n",
    "gnl = ds[[\"id\", \"n_labels\"]].groupby([\"n_labels\"]).agg([\"count\"])\n",
    "display(gnl)\n",
    "display(mm_g)\n",
    "\n",
    "u_mesh = set()\n",
    "for l in ds.labels:\n",
    "    for v in l:\n",
    "        u_mesh.add(v)\n",
    "\n",
    "print(\"unique mesh terms: \", len(u_mesh))\n",
    "del mm_g, gnl\n",
    "\n",
    "\n",
    "unique_mesh_combos = ds.labels_s.value_counts()\n",
    "print(\"unique mesh combinations:\", len(unique_mesh_combos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some random images associated to studies with n_images\n",
    "n_images = 4\n",
    "sds = ds.loc[ds.n_images == n_images, [\"id\", \"image_filename\"]].set_index(\"id\")\n",
    "print(f\"subset with exactly {n_images} images: {sds.shape}\")\n",
    "display(sds)\n",
    "\n",
    "# select n_samples random images\n",
    "n_samples = 10\n",
    "idx = random.sample(list(sds.index.values), n_samples)\n",
    "# idx = [1170,42,2177]\n",
    "\n",
    "for row in sds.loc[idx].itertuples():\n",
    "    print(f\"*** {row[0]} ***\")\n",
    "    filenames = sorted([join(ds_home, fn) for fn in row.image_filename.split(\";\")])\n",
    "    # ipyplot.plot_images(filenames, img_width=100)\n",
    "    for i, fn in enumerate(filenames):\n",
    "        print(f\"{i}/{len(filenames)}\")\n",
    "        display(Image(fn, width=224, height=224))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a binary occurrence matrix\n",
    "\n",
    "# rep_tags = np.zeros( (ds.shape[0], len(u_mesh)), dtype=int)\n",
    "# img_tags = np.zeros( (n_images, len(u_mesh)), dtype=int)\n",
    "terms = sorted(u_mesh)\n",
    "print(len(terms))\n",
    "assert \"normal\" in terms, \"normal tag missing\"\n",
    "\n",
    "matrix = []  # rows correspond to reports\n",
    "rep_matrix = []  # rows correspond to reports\n",
    "index = []\n",
    "image_report = []\n",
    "for t in ds.itertuples():\n",
    "    enc = []\n",
    "    for term in terms:\n",
    "        enc.append(term in t.labels)\n",
    "        \n",
    "    rep_matrix.append(enc)\n",
    "    for i in [fn for fn in t.image_filename.split(\";\")]:\n",
    "        index.append(i)\n",
    "        matrix.append(enc)\n",
    "        image_report.append(t.id)\n",
    "\n",
    "rep_ds = pd.DataFrame(data=np.array(rep_matrix).astype(int), columns=terms)\n",
    "rep_ds[\"id\"] = ds.id\n",
    "print(f\"dataframe report, index is report id: {rep_ds.shape}\")\n",
    "display(rep_ds)\n",
    "\n",
    "\n",
    "img_ds = pd.DataFrame(data=np.array(matrix).astype(int), columns = terms)\n",
    "img_ds[\"image_filename\"] = pd.Series(index)\n",
    "img_ds[\"report\"] = pd.Series(image_report)\n",
    "img_ds = img_ds.set_index([\"image_filename\"])\n",
    "print(f\"dataframe images, index is image_filename: {img_ds.shape}\")\n",
    "display(img_ds)\n",
    "# npterms = np.array(terms, dtype=object)\n",
    "# matrix = np.array(matrix)\n",
    "# idx = 3\n",
    "# print(matrix[idx,:])\n",
    "# print(npterms[matrix[idx,:]])\n",
    "\n",
    "\n",
    "label_counts = img_ds.sum(axis=0)\n",
    "n_labels = img_ds.sum(axis=1)\n",
    "\n",
    "print(\"n labels per images:\\n\", n_labels.value_counts())\n",
    "\n",
    "iii = n_labels == 12\n",
    "rows = img_ds.loc[iii]\n",
    "a = np.array(rows).astype(bool)\n",
    "  \n",
    "\n",
    "img_ds.to_csv(\"/mnt/datasets/uc5/std-dataset/img_ds_no_text.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspect some random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ds.loc[:, [\"id\", \"major_mesh\", \"image_filename\", \"n_images\"]])  # these are the columns used in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when there is a single image, it can be either FRONTAL or LATERAL;\n",
    "- when a report has exactly two images, the first one corresponds to a FRONTAL view and the secondo one to a LATERAL view. \n",
    "- When a report is associate to more than two images, we cannot say anything about the views of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=\"red\">select images for preparing various ecvl datasets</fond>\n",
    "\n",
    "- lateral vs frontal\n",
    "- only frontal: normal vs rest\n",
    "- normal vs rest\n",
    "- classify rest - labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "sub = ds.loc[ds.n_images == 2]\n",
    "print(sub.shape)\n",
    "frontal = []\n",
    "lateral = []\n",
    "\n",
    "def separate_images(filenames):\n",
    "    f = filenames.split(\";\")\n",
    "    assert len(f) == 2\n",
    "    frontal.append(f[0])\n",
    "    lateral.append(f[1])\n",
    "\n",
    "for row in sub.itertuples():\n",
    "    separate_images(row.image_filename)\n",
    "#subset.image_filename.apply(lambda filenames: separate_images(filenames))\n",
    "\n",
    "print(f\"frontal: {len(frontal)}\")\n",
    "print(f\"lateral: {len(lateral)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "print('frontal')\n",
    "fn = join(ds_home, random.sample(frontal, 1)[0])\n",
    "display(Image(fn, width=224, height=224))\n",
    "\n",
    "print('lateral')\n",
    "fn = join(ds_home, random.sample(lateral, 1)[0])\n",
    "display(Image(fn, width=224, height=224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ecvl_dlds(train, valid, test, y_train, y_valid, y_test):\n",
    "     d = {\n",
    "        \"name\"        : \"ECVL dataset for UC5\",\n",
    "        \"description\" : description,\n",
    "        \"classes\"     : [], \n",
    "        \"images\"      : [],\n",
    "        \"split\"       : dict(training = list(range(n_train)), \n",
    "                            validation = list(range(n_train, n_train + n_valid)), \n",
    "                            test=list(range(n_train + n_valid, len(ds))))\n",
    "    }\n",
    "\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "frontal_lab = [1, 0]\n",
    "lateral_lab = [0, 1]\n",
    "\n",
    "X = frontal + lateral\n",
    "y = [1] * len(frontal) + [2] * len(lateral)\n",
    "\n",
    "shuffle_seed = 11\n",
    "train_p = 0.7\n",
    "valid_p = 0.1\n",
    "test_p = 1 - train_p - valid_p\n",
    "print(f\"X={len(X)}, Y={len(y)}\")\n",
    "print(f\"expected train={len(X)*train_p}, val={len(X)*valid_p}, test={len(X)*test_p}\")\n",
    "\n",
    "X = np.array(frontal + lateral)\n",
    "y = np.array([1] * len(frontal) + [0] * len(lateral))\n",
    "assert X.shape[0] == 2 * sub.shape[0]\n",
    "assert y.shape[0] == 2 * sub.shape[0]\n",
    "skf= StratifiedKFold(n_splits=5, shuffle=True, random_state=shuffle_seed)\n",
    "for i, (others, test) in enumerate(skf.split(X, y)):\n",
    "    print(15 * \"=\" + f\" {i+1}/{skf.n_splits}\")\n",
    "    print(f\"train/test: label distribution, others -  {np.bincount(y[others])}   |   test -  {np.bincount(y[test])}\")\n",
    "    test_p = 1 / skf.n_splits\n",
    "    train, valid = train_test_split(others, test_size=valid_p/(1-test_p), shuffle=True, stratify=y[others], random_state=shuffle_seed+i)\n",
    "    print(f\"train/val:  label distribution, train -  {np.bincount(y[train])}   |   validation -  {np.bincount(y[valid])}\")\n",
    "    assert len(train) + len(valid) == len(others)\n",
    "    #prepare_ecvl_dlds(X_train, X_valid, X[test], y_train, y_valid, y[test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8ac5ee7dbd3222b9a621db727279133c0b9b65990c5851472ddfcb34807bcee"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('eddl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
